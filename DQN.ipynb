{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMpIvEY8gQXv",
        "outputId": "09385f0b-4da2-4ba1-8c22-0076ec7d15fb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections import deque\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "from gym import spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "d7-X6ecsgQX0",
        "outputId": "d2e9f1c1-a203-4960-c6c0-e79463a9ea3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()\n",
        "#the command below will generate an error if you haven't been addded to the wandb team yet\n",
        "#you must fill out the form posted on ed discsussion and get added to the team\n",
        "#until you are added, replace the command below by\n",
        "#run=wandb.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoJBVYJYgQX1"
      },
      "source": [
        "## DQN (Deep Q Network)\n",
        "\n",
        "In previous Labs, we have learned to use Pytorch to build deep learning models. In this lab, we will apply deep learning as function approximations in reinforcement learning.\n",
        "\n",
        "Reference: DQN https://arxiv.org/abs/1312.5602\n",
        "\n",
        "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon-$greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$\n",
        "\n",
        "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
        "\n",
        "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
        "\n",
        "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*there are alternative parameterizations here*). Let $Q_\\theta(s,a)$ be the output of the network, to estimate the optimal action value function in state $s$ and take action $a$ (and follow optimal policy thereafter).\n",
        "\n",
        "$$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R972TxQlgQX3"
      },
      "source": [
        "### Bellman optimality equation\n",
        "\n",
        "We will use Bellman optimality equation to find $\\theta$ such that the above approximation holds better. Recall that for optimal Q function $Q^\\ast(s,a)$ the following holds for all $(s,a)$\n",
        "\n",
        "$$Q^\\ast(s_t,a_t) = \\mathbb{E}\\big[r_t + \\gamma \\max_a Q^\\ast(s_{t+1},a)\\big]$$\n",
        "\n",
        "where the expectation is wrt the random reward $r_t$ and transition to the next state $s_{t+1}$. A natural objective to consider is\n",
        "\n",
        "$$\\min_\\theta\\  (Q_\\theta(s_t,a_t) -\\mathbb{E}\\big[r_t + \\gamma  \\max_a  Q_{\\hat \\theta}(s_{t+1},a)\\big])^2$$\n",
        "at the current or previous $\\hat \\theta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD-rkp3sgQX4"
      },
      "source": [
        "### Building the DQN model\n",
        "\n",
        "The first step is to build a neural network with parameters $\\theta$ that predicts $Q_\\theta(s,a)$ for any $(s,a)$. You can either build a network that\n",
        "\n",
        "* (in case of small number $K$ of discrete actions) takes as input a  representation of state $s$ and outputs a $K$-dimensional vector giving scores $Q(s,a), a=1,\\ldots, K$ for all actions\n",
        "\n",
        "or\n",
        "\n",
        "* takes as input a concatenated representation of state and action $(s,a)$ and output one dimensional score $Q_\\theta(s,a)$,\n",
        "\n",
        "Below we have provided a skeleton code (incomplete) for defining and training the Q-function. **You need to fill in the DNN model definition and loss function definition**. Refer to regression lab (lab 2) for help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LhmRGDjHgQX5"
      },
      "outputs": [],
      "source": [
        "# define neural net Q_\\theta(s,a) as a class\n",
        "\n",
        "class Qfunction(object):\n",
        "\n",
        "    def __init__(self, obssize, actsize, lr, device, loss='huber', opt='adam'):\n",
        "        \"\"\"\n",
        "        obssize: dimension of state space\n",
        "        actsize: dimension of action space\n",
        "        sess: sess to execute this Qfunction\n",
        "        optimizer:\n",
        "        \"\"\"\n",
        "        # DEFINE THE MODEL\n",
        "        self.model = torch.nn.Sequential(\n",
        "                    #TODO\n",
        "                    #input layer\n",
        "                    torch.nn.Linear(obssize, 512),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Linear(512, 256),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Linear(256, 128),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Linear(128, actsize)\n",
        "                ).to(device)\n",
        "\n",
        "        # DEFINE THE OPTIMIZER\n",
        "        if opt == 'adam':\n",
        "          self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        elif opt == 'rmsprop':\n",
        "          self.optimizer = torch.optim.RMSprop(self.model.parameters(), lr=lr, alpha=0.95, eps=0.01)\n",
        "        elif opt == 'adamw':\n",
        "          self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, amsgrad=True)\n",
        "\n",
        "        # RECORD HYPER-PARAMS\n",
        "        self.obssize = obssize\n",
        "        self.actsize = actsize\n",
        "        self.device = device\n",
        "        self.loss = loss\n",
        "\n",
        "    def _to_one_hot(self, y, num_classes):\n",
        "        \"\"\"\n",
        "        convert an integer vector y into one-hot representation\n",
        "        \"\"\"\n",
        "        scatter_dim = len(y.size())\n",
        "        y_tensor = y.view(*y.size(), -1)\n",
        "        zeros = torch.zeros(*y.size(), num_classes, dtype=y.dtype, device=self.device)\n",
        "        return zeros.scatter(scatter_dim, y_tensor, 1)\n",
        "\n",
        "\n",
        "    def compute_Qvalues(self, states, actions):\n",
        "        \"\"\"\n",
        "        input: list of numsamples state-action pairs\n",
        "        output: List of Q values for each input (s,a). The output will have size [numsamples, 1]\n",
        "        \"\"\"\n",
        "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
        "        #This will be different for neural network that takes as input a state-action pair\n",
        "\n",
        "        try:\n",
        "          states = torch.FloatTensor(states).to(self.device)\n",
        "        except TypeError:\n",
        "          pass\n",
        "        q_preds = self.model(states)\n",
        "        action_onehot = self._to_one_hot(actions.to(self.device), actsize)\n",
        "        q_preds_selected = torch.sum(q_preds * action_onehot, axis=-1)\n",
        "\n",
        "        return q_preds_selected\n",
        "\n",
        "    def compute_maxQvalues(self, states):\n",
        "        \"\"\"\n",
        "        input: a list of numsamples states\n",
        "        output: max_a Q(s,a) values for every input state s in states. The output will have size numsamples\n",
        "        \"\"\"\n",
        "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
        "        #if the neural takes as input a state-action pair, then the code will need to loop over all actions to compute all values\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        Qvalues = self.model(states)\n",
        "        q_preds_greedy = Qvalues.max(1).values\n",
        "\n",
        "        return q_preds_greedy\n",
        "\n",
        "    def compute_argmaxQ(self, state):\n",
        "        \"\"\"\n",
        "        input: one state s\n",
        "        output: arg max_a Q(self.model(states).cpu().data.numpy()s,a) values for the input state s. The output will have size 1\n",
        "        \"\"\"\n",
        "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
        "        #if the neural takes as input a state-action pair, then the code will need to loop over all actions to compute all values\n",
        "\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        Qvalue = self.model(state)\n",
        "        greedy_action = Qvalue.argmax().item()\n",
        "\n",
        "        return greedy_action\n",
        "\n",
        "    def take_action(self, state, possible_actions, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "          return np.random.choice(possible_actions)\n",
        "        else:\n",
        "          return self.compute_argmaxQ(state)\n",
        "\n",
        "\n",
        "    def train(self, states, actions, targets, verbose=False):\n",
        "        \"\"\"\n",
        "        states: numpy array as input to compute loss (s)\n",
        "        actions: numpy array as input to compute loss (a)\n",
        "        targets: numpy array as input to compute loss (Q targets)\n",
        "        \"\"\"\n",
        "        states = torch.from_numpy(states).float().to(device)\n",
        "        actions = torch.from_numpy(actions).long().to(device)\n",
        "        targets = torch.from_numpy(targets).float().to(device)\n",
        "\n",
        "        # COMPUTE Q PREDICTIONS for all state-action pairs\n",
        "        q_preds_selected = self.compute_Qvalues(states, actions)\n",
        "\n",
        "        # LOSS\n",
        "        if verbose: print(q_preds_selected.shape, targets.shape)\n",
        "        if self.loss == 'mse':\n",
        "          loss = torch.mean((q_preds_selected - targets)**2)\n",
        "        elif self.loss == 'huber':\n",
        "          loss_func = torch.nn.SmoothL1Loss()\n",
        "          loss = loss_func(q_preds_selected, targets)\n",
        "\n",
        "        # BACKWARD PASS\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # UPDATE\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.detach().cpu().data.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_wC8o5r5ws3H"
      },
      "outputs": [],
      "source": [
        "\n",
        "BASE_PATH = '/Users/amanchopra/Documents/School/MS/Fall 2023/RL/Final Project/Data/Processed'\n",
        "data = pd.read_csv(f\"{BASE_PATH}/transition_model_90_min_history_rewards.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "jQlRoNm5w0N8",
        "outputId": "c423498f-5611-4624-9db0-44dd3720fc13"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BG_t0</th>\n",
              "      <th>BG_t1</th>\n",
              "      <th>BG_t2</th>\n",
              "      <th>BG_t3</th>\n",
              "      <th>BG_t4</th>\n",
              "      <th>BG_t5</th>\n",
              "      <th>BG_t6</th>\n",
              "      <th>BG_t7</th>\n",
              "      <th>BG_t8</th>\n",
              "      <th>BG_t9</th>\n",
              "      <th>...</th>\n",
              "      <th>IOB_t28</th>\n",
              "      <th>IOB_t29</th>\n",
              "      <th>IOB_t30</th>\n",
              "      <th>IOB_t31</th>\n",
              "      <th>IOB_t32</th>\n",
              "      <th>IOB_t33</th>\n",
              "      <th>IOB_t34</th>\n",
              "      <th>IOB_t35</th>\n",
              "      <th>simple_reward</th>\n",
              "      <th>magni_reward</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>127.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.491872</td>\n",
              "      <td>0.502033</td>\n",
              "      <td>0.510965</td>\n",
              "      <td>0.531246</td>\n",
              "      <td>0.543538</td>\n",
              "      <td>0.555829</td>\n",
              "      <td>0.568121</td>\n",
              "      <td>0.580412</td>\n",
              "      <td>0</td>\n",
              "      <td>-10.211281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>132.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.471303</td>\n",
              "      <td>0.491872</td>\n",
              "      <td>0.502033</td>\n",
              "      <td>0.510965</td>\n",
              "      <td>0.531246</td>\n",
              "      <td>0.543538</td>\n",
              "      <td>0.555829</td>\n",
              "      <td>0.568121</td>\n",
              "      <td>0</td>\n",
              "      <td>-12.579462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.469787</td>\n",
              "      <td>0.471303</td>\n",
              "      <td>0.491872</td>\n",
              "      <td>0.502033</td>\n",
              "      <td>0.510965</td>\n",
              "      <td>0.531246</td>\n",
              "      <td>0.543538</td>\n",
              "      <td>0.555829</td>\n",
              "      <td>0</td>\n",
              "      <td>-12.139707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>120.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.458479</td>\n",
              "      <td>0.469787</td>\n",
              "      <td>0.471303</td>\n",
              "      <td>0.491872</td>\n",
              "      <td>0.502033</td>\n",
              "      <td>0.510965</td>\n",
              "      <td>0.531246</td>\n",
              "      <td>0.543538</td>\n",
              "      <td>0</td>\n",
              "      <td>-12.800759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>106.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.444016</td>\n",
              "      <td>0.458479</td>\n",
              "      <td>0.469787</td>\n",
              "      <td>0.471303</td>\n",
              "      <td>0.491872</td>\n",
              "      <td>0.502033</td>\n",
              "      <td>0.510965</td>\n",
              "      <td>0.531246</td>\n",
              "      <td>0</td>\n",
              "      <td>-14.602987</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 111 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   BG_t0  BG_t1  BG_t2  BG_t3  BG_t4  BG_t5  BG_t6  BG_t7  BG_t8  BG_t9  ...  \\\n",
              "0  127.0  121.0  113.0   99.0   95.0   97.0  101.0  107.0  111.0  118.0  ...   \n",
              "1  132.0  127.0  121.0  113.0   99.0   95.0   97.0  101.0  107.0  111.0  ...   \n",
              "2  130.0  132.0  127.0  121.0  113.0   99.0   95.0   97.0  101.0  107.0  ...   \n",
              "3  120.0  130.0  132.0  127.0  121.0  113.0   99.0   95.0   97.0  101.0  ...   \n",
              "4  106.0  120.0  130.0  132.0  127.0  121.0  113.0   99.0   95.0   97.0  ...   \n",
              "\n",
              "    IOB_t28   IOB_t29   IOB_t30   IOB_t31   IOB_t32   IOB_t33   IOB_t34  \\\n",
              "0  0.491872  0.502033  0.510965  0.531246  0.543538  0.555829  0.568121   \n",
              "1  0.471303  0.491872  0.502033  0.510965  0.531246  0.543538  0.555829   \n",
              "2  0.469787  0.471303  0.491872  0.502033  0.510965  0.531246  0.543538   \n",
              "3  0.458479  0.469787  0.471303  0.491872  0.502033  0.510965  0.531246   \n",
              "4  0.444016  0.458479  0.469787  0.471303  0.491872  0.502033  0.510965   \n",
              "\n",
              "    IOB_t35  simple_reward  magni_reward  \n",
              "0  0.580412              0    -10.211281  \n",
              "1  0.568121              0    -12.579462  \n",
              "2  0.555829              0    -12.139707  \n",
              "3  0.543538              0    -12.800759  \n",
              "4  0.531246              0    -14.602987  \n",
              "\n",
              "[5 rows x 111 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qFfiHXqx_Hj",
        "outputId": "ce024a20-17c1-49a2-fa25-8e9945258633"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "simple_reward\n",
              " 0     23812\n",
              " 1     18604\n",
              "-10     3301\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['simple_reward'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    45717.000000\n",
              "mean        -8.245843\n",
              "std         11.819608\n",
              "min        -84.319007\n",
              "25%         -8.024595\n",
              "50%         -5.669213\n",
              "75%         -1.246025\n",
              "max         -0.000047\n",
              "Name: magni_reward, dtype: float64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['magni_reward'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC4rGthegQX9"
      },
      "source": [
        "At this point you can skip ahead to implementing the basic Q-learning that at every step $t$ in the environment\n",
        "* given state $s_t$, computes greedy actions from Q-values (using compute_argmaxQ function above) and uses $\\epsilon$-greedy select an action $a_t$,\n",
        "* makes observation of reward $r_t$ and next state $s_{t+1}$\n",
        "* using compute_maxQvalues() function, computes target\n",
        "  $$r_t + \\gamma \\max_a Q_\\theta(s_{t+1},a)$$\n",
        "and then retrains the Q-function using train() function above (with numsamples=1)\n",
        "\n",
        "However, for improved performance you may want to consider ideas like batch training (numsamples>1 is the batch size) with experience replay buffer and target-networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8RupUgMgQX-"
      },
      "source": [
        "**Replay Buffer**\n",
        "\n",
        "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$, when we minimize the Bellman error. When optimizing the Bellman error loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples from buffer $(s_i,a_i,r_i,s_{i}') \\sim R$ and then compute\n",
        "targets\n",
        "\n",
        "$$d_i=r_i + \\max_a \\gamma Q_{\\theta}(s_i^\\prime,a)$$\n",
        "for all $i$. Use the above training function train() with input as list $(s_i, a_i, d_i)_{i=1}^N$  to update parameters using backprop.\n",
        "\n",
        "**Target Network**\n",
        "\n",
        "Maintain a target network in addition to the original pricipal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta^-$ is copied from the principal network every $\\tau$ time steps. Target network is used to compute the targets for update\n",
        "\n",
        "$$d_i =  r_t + \\gamma \\max_a Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
        "\n",
        "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sIOVd8pgQX_"
      },
      "source": [
        "Hence several critical parts of the complete pseudocode for DQN is as follows:\n",
        "\n",
        "**Initialization.**\n",
        "principal network $Q_\\theta(s,a)$, target network $Q_{\\theta^{-}}(s,a)$. Replay buffer $R = \\{\\}$ (empty).\n",
        "\n",
        "**At each time step $t.$**\n",
        "The agent executes action using $\\epsilon-$greedy based on the principal network $Q_\\theta(s,a)$. To update $\\theta$: sample $N$ tuples $(s_i,a_i,r_i,s_i^\\prime) \\sim R$, compute empirical loss\n",
        "\n",
        "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - (r_i + \\gamma \\max_a Q_{\\theta^{-}}(s_i^\\prime,a))^2$$\n",
        "\n",
        "and update parameter $\\theta$ using backprop (just take one gradient step).\n",
        "\n",
        "**Update target network.**\n",
        "Every $\\tau$ time steps, update target network by copying $\\theta_{\\text{target}} \\leftarrow \\theta$.\n",
        "\n",
        "**Bellman target.**\n",
        "Above, we have defined the target values as being computed from a target net with parameter $\\theta^-$\n",
        "$$r_i + \\gamma \\max_a Q_{\\theta^{-}}(s_i^\\prime,a)$$\n",
        "It is worth thinking about what happens if we are at the end of an episode, that is, what if $s_i^\\prime$ here is a terminal state. In this case, should the Bellman error be defined exactly the same as above? Do we need some modifications? Think carefully about this as this will greatly impact the algorithmic performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6bTnNEpgQX_"
      },
      "source": [
        "### Implementation of replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJnxONjRgQYA",
        "outputId": "044012f9-30e6-486b-9c8f-89856230ebc2"
      },
      "outputs": [],
      "source": [
        "# Implement replay buffer\n",
        "import random\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    def __init__(self, maxlength):\n",
        "        \"\"\"\n",
        "        maxlength: max number of tuples to store in the buffer\n",
        "        if there are more tuples than maxlength, pop out the oldest tuples\n",
        "        \"\"\"\n",
        "        self.buffer = deque()\n",
        "        self.number = 0\n",
        "        self.maxlength = maxlength\n",
        "\n",
        "    def append(self, experience):\n",
        "        \"\"\"\n",
        "        this function implements appending new experience tuple\n",
        "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
        "        \"\"\"\n",
        "        self.buffer.append(experience)\n",
        "        self.number += 1\n",
        "\n",
        "    def pop(self):\n",
        "        \"\"\"\n",
        "        pop out the oldest tuples if self.number > self.maxlength\n",
        "        \"\"\"\n",
        "        while self.number > self.maxlength:\n",
        "            self.buffer.popleft()\n",
        "            self.number -= 1\n",
        "\n",
        "    def sample(self, batchsize):\n",
        "        \"\"\"\n",
        "        this function samples 'batchsize' experience tuples\n",
        "        batchsize: size of the minibatch to be sampled\n",
        "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
        "        \"\"\"\n",
        "        minibatch = random.sample(self.buffer,batchsize)\n",
        "        return minibatch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXJfV58XgQYA"
      },
      "source": [
        "### Code snippet for copying target network\n",
        "You may use th following to update target network i.e. to copy from principal network to target network. We need to use tensorflow scope to distinguish the computational graphs of target and principal networks. The following function builds a tensorflow operation that does the copying $\\theta^- \\leftarrow \\theta$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sn2ptEZwgQYB"
      },
      "outputs": [],
      "source": [
        "def run_target_update(Qprincipal, Qtarget):\n",
        "    for v,v_ in zip(Qprincipal.model.parameters(), Qtarget.model.parameters()):\n",
        "        v_.data.copy_(v.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wgIE5aJgQYB"
      },
      "source": [
        "## Main code for DQN\n",
        "Now that we have all the ingredients for DQN, we can write the main procedure to train DQN on a given environment. The implementation is straightforward if you follow the pseudocode pdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "QuNNnNlgxVjv"
      },
      "outputs": [],
      "source": [
        "class CustomEnv(gym.Env):\n",
        "    def __init__(self, dataframe, reward='simple_reward'):\n",
        "        super(CustomEnv, self).__init__()\n",
        "        self.df = dataframe\n",
        "        \n",
        "        feature_dim = int((max([int(col[4:]) for col in dataframe.columns if 'BG' in col]) + 1) / 2)\n",
        "\n",
        "        self.s_cols = [f'BG_t{i}' for i in range(feature_dim)] + [f'HR_t{i}' for i in range(feature_dim)] + [f'IOB_t{i}' for i in range(feature_dim)]\n",
        "        self.a_col = 'InsulinDelivered'\n",
        "        self.s_prime_cols = [f'BG_t{i}' for i in range(feature_dim, 2*feature_dim)] + [f'HR_t{i}' for i in range(feature_dim, 2*feature_dim)] + [f'IOB_t{i}' for i in range(feature_dim, 2*feature_dim)]\n",
        "        self.reward_col = reward\n",
        "\n",
        "        actions = dataframe[self.a_col]\n",
        "        self.action_space = spaces.Box(low=np.array([actions.min()]), high=np.array([actions.max()]))\n",
        "\n",
        "        bg_min = min([data[f'BG_t{i}'].min() for i in range(feature_dim*2)])\n",
        "        bg_max = max([data[f'BG_t{i}'].max() for i in range(feature_dim*2)])\n",
        "        hr_min = min([data[f'HR_t{i}'].min() for i in range(feature_dim*2)])\n",
        "        hr_max = max([data[f'HR_t{i}'].max() for i in range(feature_dim*2)])\n",
        "        iob_min = min([data[f'IOB_t{i}'].min() for i in range(feature_dim*2)])\n",
        "        iob_max = max([data[f'IOB_t{i}'].max() for i in range(feature_dim*2)])\n",
        "        \n",
        "        self.observation_space = spaces.Box(low=np.array([bg_min]*feature_dim + [hr_min]*feature_dim + [iob_min]*feature_dim), high=np.array([bg_max]*feature_dim + [hr_max]*feature_dim + [iob_max]*feature_dim))\n",
        "        self.current_state_ind = None\n",
        "\n",
        "        \n",
        "    def reset(self, seed=None):\n",
        "        if seed:\n",
        "            sample = self.df.sample(1, random_state=seed)\n",
        "            s_ind = sample.index[0]\n",
        "            s = sample[self.s_cols].values.flatten()\n",
        "        else:\n",
        "            s = self.df.sample(1)\n",
        "            s_ind = sample.index[0]\n",
        "            s = sample[self.s_cols].values.flatten()\n",
        "        self.current_state_ind = s_ind\n",
        "        return s\n",
        "\n",
        "    def step(self, action):\n",
        "        # get the next state and reward based on the current state and action\n",
        "        next_state = self.iloc[self.current_state_ind]\n",
        "        if next_state[self.a_col] != action:\n",
        "            raise Exception(f\"Cannot take action (InsulinDelivered={action}) as this action doesn't exist for the current state at index {self.current_state_ind}!\")\n",
        "        next_state  = next_state[self.current_state_ind][self.s_prime_cols].values\n",
        "        reward = next_state[self.current_state_ind][self.reward_col]\n",
        "        done = self.is_done()\n",
        "        info = {}\n",
        "        self.current_state_ind = \n",
        "\n",
        "        return next_state, reward, done, info\n",
        "    \n",
        "    def does_action_exist(self, action):\n",
        "        if action in self.df[self.a_col]:\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def get_closest_action(self, action):\n",
        "        ind = np.random.choice(np.where((self.df[self.a_col] - action).abs() == (self.df[self.a_col] - action).abs().min())[0])\n",
        "        return data.iloc[ind][self.a_col]\n",
        "\n",
        "    def is_done(self):\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "i3zUTBupxbih"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(54,)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "('Lengths must match to compare', (45717,), (1,))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/amanchopra/Documents/School/MS/Fall 2023/RL/Final Project/Code/DQN.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(observation\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m next_observation, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n",
            "\u001b[1;32m/Users/amanchopra/Documents/School/MS/Fall 2023/RL/Final Project/Code/DQN.ipynb Cell 25\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# get the next state and reward based on the current state and action\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m next_state, reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlookup_next_state_and_reward(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_done()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m info \u001b[39m=\u001b[39m {}\n",
            "\u001b[1;32m/Users/amanchopra/Documents/School/MS/Fall 2023/RL/Final Project/Code/DQN.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m iob_col \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIOB_t\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Look up the next state and reward\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m next_state_row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mloc[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[hr_col] \u001b[39m==\u001b[39m current_state[\u001b[39m0\u001b[39m]) \u001b[39m&\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[bg_col] \u001b[39m==\u001b[39m current_state[\u001b[39m1\u001b[39m]) \u001b[39m&\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[iob_col] \u001b[39m==\u001b[39m current_state[\u001b[39m2\u001b[39m]) \u001b[39m&\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf[\u001b[39m'\u001b[39;49m\u001b[39mInsulinDelivered\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m insulin_delivered)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m next_state \u001b[39m=\u001b[39m next_state_row[[hr_col, bg_col, iob_col]]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X25sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# print(next_state)\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/rl/lib/python3.10/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/rl/lib/python3.10/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/rl/lib/python3.10/site-packages/pandas/core/series.py:5799\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5796\u001b[0m lvalues \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[1;32m   5797\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 5799\u001b[0m res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[1;32m   5801\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/rl/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:323\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rvalues, (np\u001b[39m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[1;32m    319\u001b[0m     \u001b[39m# TODO: make this treatment consistent across ops and classes.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[39m#  We are not catching all listlikes here (e.g. frozenset, tuple)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[39m#  The ambiguous case is object-dtype.  See GH#27803\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lvalues) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(rvalues):\n\u001b[0;32m--> 323\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLengths must match to compare\u001b[39m\u001b[39m\"\u001b[39m, lvalues\u001b[39m.\u001b[39mshape, rvalues\u001b[39m.\u001b[39mshape\n\u001b[1;32m    325\u001b[0m         )\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    328\u001b[0m     (\u001b[39misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[39mor\u001b[39;00m right \u001b[39mis\u001b[39;00m NaT)\n\u001b[1;32m    329\u001b[0m     \u001b[39mand\u001b[39;00m lvalues\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m \u001b[39mobject\u001b[39m\n\u001b[1;32m    330\u001b[0m ):\n\u001b[1;32m    331\u001b[0m     \u001b[39m# Call the method on lvalues\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     res_values \u001b[39m=\u001b[39m op(lvalues, rvalues)\n",
            "\u001b[0;31mValueError\u001b[0m: ('Lengths must match to compare', (45717,), (1,))"
          ]
        }
      ],
      "source": [
        "env = CustomEnv(data)\n",
        "observation = env.reset()\n",
        "print(observation.shape)\n",
        "action = env.action_space.sample()\n",
        "next_observation, reward, done, info = env.step(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "45219cb311904b08aa78d0c5765e99c5",
            "7b0a959e6a774a72ad2e7539d9bca64a",
            "290e12287bbe4f22a3798e791169b274",
            "d49640d6db1b467badf1774f074c8bd5",
            "b8eeb5b7f19f4ff49808a41c3b268f38",
            "190a0163792e4038a9d778b7a943b6aa",
            "f20f21a89c604d239fd23f3420c9e77e",
            "bd96d67c8f304cd3991b3dc0ada24776"
          ]
        },
        "id": "LHlgGA9_gQYB",
        "outputId": "4b5b2662-f946-4fe3-ce77-175b4f7b66c2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:6w65n4aw) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45219cb311904b08aa78d0c5765e99c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.010 MB uploaded\\r'), FloatProgress(value=0.11213168424881775, max=1.…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">colorful-energy-1</strong> at: <a href='https://wandb.ai/rl23/rl-cartpole/runs/6w65n4aw' target=\"_blank\">https://wandb.ai/rl23/rl-cartpole/runs/6w65n4aw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20231203_174120-6w65n4aw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:6w65n4aw). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231203_174238-k2uep8fk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rl23/rl-cartpole/runs/k2uep8fk' target=\"_blank\">noble-smoke-2</a></strong> to <a href='https://wandb.ai/rl23/rl-cartpole' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rl23/rl-cartpole' target=\"_blank\">https://wandb.ai/rl23/rl-cartpole</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rl23/rl-cartpole/runs/k2uep8fk' target=\"_blank\">https://wandb.ai/rl23/rl-cartpole/runs/k2uep8fk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "buffersize 35\n",
            "episode 0 ave training returns 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-3ff73737c084>:75: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  states = torch.FloatTensor(states).to(self.device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "buffersize 385\n",
            "episode 10 ave training returns 0.0\n",
            "buffersize 735\n",
            "episode 20 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 30 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 40 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 50 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 60 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 70 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 80 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 90 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 100 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 110 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 120 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 130 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 140 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 150 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 160 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 170 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 180 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 190 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 200 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 210 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 220 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 230 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 240 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 250 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 260 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 270 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 280 ave training returns 0.0\n",
            "buffersize 1000\n",
            "episode 290 ave training returns 0.0\n"
          ]
        }
      ],
      "source": [
        "#%%wandb\n",
        "#remove above line if you do not want to see inline plots from wandb\n",
        "\n",
        "# hyper-parameters\n",
        "lr = 1e-3  # learning rate for gradient update\n",
        "batchsize = 64  # batchsize for buffer sampling\n",
        "maxlength = 1000  # max number of tuples held by buffer\n",
        "envname = \"CartPole-v0\"  # environment name\n",
        "tau = 100  # time steps for target update\n",
        "episodes = 300  # number of episodes to run\n",
        "initialsize = 500  # initial time steps before start training\n",
        "epsilon = .2  # constant for exploration\n",
        "gamma = .99  # discount\n",
        "q_update_num_steps = 1 # time steps for q network update\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "verbose = True\n",
        "opt = 'adamw'\n",
        "loss = 'huber'\n",
        "\n",
        "run = wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"rl-cartpole\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"method\": \"dqn\",\n",
        "    \"lr\": lr,\n",
        "    \"buffer_batchsize\": batchsize,\n",
        "    \"buffer_maxlength\": maxlength,\n",
        "    \"tau\": tau,\n",
        "    \"episodes\": episodes,\n",
        "    \"initial_size\": initialsize,\n",
        "    \"epsilon\": epsilon,\n",
        "    \"gamma\": gamma,\n",
        "    \"q_update_num_steps\": q_update_num_steps,\n",
        "    \"device\": device,\n",
        "    \"optimizer\": opt,\n",
        "    \"loss\": loss\n",
        "    }\n",
        ")\n",
        "\n",
        "# initialize environment\n",
        "# env = gym.make(envname)\n",
        "env = CustomEnv(data)\n",
        "obssize = env.observation_space.low.size\n",
        "actsize = env.action_space.n\n",
        "\n",
        "# initialize Q-function networks (princpal and target)\n",
        "Qprincipal = Qfunction(obssize, actsize, lr, device, opt=opt, loss=loss)\n",
        "Qtarget = Qfunction(obssize, actsize, lr, device, opt=opt, loss=loss)\n",
        "\n",
        "# initialization of graph and buffer\n",
        "buffer = ReplayBuffer(maxlength)\n",
        "\n",
        "# main iteration\n",
        "rrecord = []\n",
        "totalstep = 0\n",
        "\n",
        "for episode in range(episodes):\n",
        "\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    rsum = 0\n",
        "    eps_actions = {i: 0 for i in range(env.action_space.n)}\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        #greedy choice below. Use epsilon greedy for exploration\n",
        "        action = Qprincipal.take_action(np.expand_dims(obs,0), [i for i in range(env.action_space.n)], epsilon)\n",
        "        eps_actions[action] += 1\n",
        "\n",
        "        newobs, r, done, _ = env.step(action)\n",
        "        done_ = 1 if done else 0\n",
        "        e = (obs, action, r, done_, newobs)\n",
        "\n",
        "        #IF NOT USING BUFFER:\n",
        "        #use single sample (obs, action, r, done_, newobs) with Qtarget to compute target and train Qprincipal\n",
        "\n",
        "        # ELSE IF USING REPLAY BUFFER\n",
        "        # append experiences e to buffer\n",
        "\n",
        "        buffer.append(e)\n",
        "        buffer.pop()\n",
        "\n",
        "        #every few episodes (decide the frequency) sample a minibatch from buffer\n",
        "        #compute targets in batch using Qtarget and train  Qprincipal\n",
        "        if totalstep % q_update_num_steps == 0 and buffer.number >= batchsize:\n",
        "          samples = buffer.sample(batchsize)\n",
        "          states = [e[0] for e in samples]\n",
        "          actions = [e[1] for e in samples]\n",
        "          rewards = [e[2] for e in samples]\n",
        "          dones = np.array([e[3] for e in samples], dtype=bool)\n",
        "          next_states = [e[4] for e in samples]\n",
        "\n",
        "          non_ternimal_next_states = [next_state for next_state, done in zip(next_states, dones) if not done]\n",
        "          non_ternimal_next_states_values = Qtarget.compute_maxQvalues(non_ternimal_next_states)\n",
        "\n",
        "          next_states_values = np.zeros(batchsize)\n",
        "          next_states_values[~dones] = non_ternimal_next_states_values.cpu().data.numpy()\n",
        "\n",
        "          #next_states_values = [0 if dones[i] else next(non_ternimal_next_states_values) for i in range(batchsize)]\n",
        "\n",
        "          targets = [rewards[i] if dones[i] else rewards[i] + gamma * next_states_values[i] for i in range(batchsize)]\n",
        "          Qprincipal.train(np.array(states), np.array(actions), np.array(targets), verbose=False)\n",
        "\n",
        "\n",
        "        #UPDATE target network\n",
        "        #every tau steps update copy the principal network to the target network\n",
        "        if totalstep % tau == 0:\n",
        "            run_target_update(Qprincipal, Qtarget)\n",
        "\n",
        "        # update\n",
        "        totalstep += 1\n",
        "        rsum += r\n",
        "        obs = newobs\n",
        "\n",
        "    rrecord.append(rsum)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # printing functions for debugging purposes. Feel free to add more\n",
        "    if verbose and episode % 10 == 0:\n",
        "       print('buffersize {}'.format(buffer.number))\n",
        "       print('episode {} ave training returns {}'.format(episode, np.mean(rrecord[-10:])))\n",
        "\n",
        "    #printing moving averages for smoothed visualization.\n",
        "    fixedWindow=100\n",
        "    movingAverage=0\n",
        "    if len(rrecord) >= fixedWindow:\n",
        "        movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
        "\n",
        "    wandb.log({ \"training reward\" : rsum, \"train reward moving average\" : movingAverage})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGtiM-vmgQYC"
      },
      "source": [
        "Finally, we evaluate the performance of the trained agent. We will evaluate the performance of the greedy policy wrt learned Q-function. The evaluation will be run 10 times, each for eval_epsiodes and print out the average performance across these episodes. Please **do not** change the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "L55512RpgQYC"
      },
      "outputs": [],
      "source": [
        "### DO NOT CHANGE\n",
        "def evaluate(Q, env, episodes):\n",
        "    # main iteration\n",
        "    score = 0.0\n",
        "    for episode in range(episodes):\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        rsum = 0\n",
        "\n",
        "        while not done:\n",
        "            # always greedy\n",
        "            action = Q.compute_argmaxQ(np.expand_dims(obs,0))\n",
        "\n",
        "\n",
        "            # mdp stepping forward\n",
        "            newobs, r, done, _ = env.step(action)\n",
        "\n",
        "            # update data\n",
        "            rsum += r\n",
        "            obs = newobs\n",
        "\n",
        "\n",
        "        wandb.log({\"eval reward\" : rsum})\n",
        "        score = score + rsum\n",
        "    score = score/episodes\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9-OyN_1gQYD",
        "outputId": "4f904521-41db-4cab-9f0a-19e8b9b76141"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval performance of DQN agent: 0.0\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE CODE HERE\n",
        "# after training, we will evaluate the performance of the agent\n",
        "# on a target environment\n",
        "# env_test = gym.make(envname)\n",
        "env_test = CustomEnv(data)\n",
        "eval_episodes = 1000\n",
        "score = evaluate(Qprincipal, env_test, eval_episodes)\n",
        "wandb.run.summary[\"score\"]=score\n",
        "\n",
        "print(\"eval performance of DQN agent: {}\".format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574,
          "referenced_widgets": [
            "eb7e5dd479084da08180b70fba0d3e44",
            "1d6754381337431195a2fafe3efa9483",
            "adabf5316aef42cd95cf88553fd42656",
            "a27b3a580aca4773b0b7fdc27af55df8",
            "0045943bff2e4aa9b1d168b1661e4f64",
            "9b184009643841869af4093d187c8529",
            "1e458618cbb847ceb470a4383c117adc",
            "aa85385400da44f588ae4b52c9bdb15c"
          ]
        },
        "id": "VBt7yCCygQYD",
        "outputId": "b481f122-855e-490e-fe58-4dafbcbb24b5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb7e5dd479084da08180b70fba0d3e44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train reward moving average</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval reward</td><td>0</td></tr><tr><td>score</td><td>0.0</td></tr><tr><td>train reward moving average</td><td>0.0</td></tr><tr><td>training reward</td><td>0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">noble-smoke-2</strong> at: <a href='https://wandb.ai/rl23/rl-cartpole/runs/k2uep8fk' target=\"_blank\">https://wandb.ai/rl23/rl-cartpole/runs/k2uep8fk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20231203_174238-k2uep8fk/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0045943bff2e4aa9b1d168b1661e4f64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "190a0163792e4038a9d778b7a943b6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d6754381337431195a2fafe3efa9483": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0045943bff2e4aa9b1d168b1661e4f64",
            "placeholder": "​",
            "style": "IPY_MODEL_9b184009643841869af4093d187c8529",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "1e458618cbb847ceb470a4383c117adc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "290e12287bbe4f22a3798e791169b274": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f20f21a89c604d239fd23f3420c9e77e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd96d67c8f304cd3991b3dc0ada24776",
            "value": 1
          }
        },
        "45219cb311904b08aa78d0c5765e99c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b0a959e6a774a72ad2e7539d9bca64a",
              "IPY_MODEL_290e12287bbe4f22a3798e791169b274"
            ],
            "layout": "IPY_MODEL_d49640d6db1b467badf1774f074c8bd5"
          }
        },
        "7b0a959e6a774a72ad2e7539d9bca64a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8eeb5b7f19f4ff49808a41c3b268f38",
            "placeholder": "​",
            "style": "IPY_MODEL_190a0163792e4038a9d778b7a943b6aa",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "9b184009643841869af4093d187c8529": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a27b3a580aca4773b0b7fdc27af55df8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa85385400da44f588ae4b52c9bdb15c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "adabf5316aef42cd95cf88553fd42656": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e458618cbb847ceb470a4383c117adc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa85385400da44f588ae4b52c9bdb15c",
            "value": 1
          }
        },
        "b8eeb5b7f19f4ff49808a41c3b268f38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd96d67c8f304cd3991b3dc0ada24776": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d49640d6db1b467badf1774f074c8bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7e5dd479084da08180b70fba0d3e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d6754381337431195a2fafe3efa9483",
              "IPY_MODEL_adabf5316aef42cd95cf88553fd42656"
            ],
            "layout": "IPY_MODEL_a27b3a580aca4773b0b7fdc27af55df8"
          }
        },
        "f20f21a89c604d239fd23f3420c9e77e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
