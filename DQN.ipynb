{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMpIvEY8gQXv",
        "outputId": "09385f0b-4da2-4ba1-8c22-0076ec7d15fb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections import deque\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "from gym import spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "d7-X6ecsgQX0",
        "outputId": "d2e9f1c1-a203-4960-c6c0-e79463a9ea3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamanichopra\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()\n",
        "#the command below will generate an error if you haven't been addded to the wandb team yet\n",
        "#you must fill out the form posted on ed discsussion and get added to the team\n",
        "#until you are added, replace the command below by\n",
        "#run=wandb.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoJBVYJYgQX1"
      },
      "source": [
        "## DQN (Deep Q Network)\n",
        "\n",
        "In previous Labs, we have learned to use Pytorch to build deep learning models. In this lab, we will apply deep learning as function approximations in reinforcement learning.\n",
        "\n",
        "Reference: DQN https://arxiv.org/abs/1312.5602\n",
        "\n",
        "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon-$greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$\n",
        "\n",
        "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
        "\n",
        "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
        "\n",
        "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*there are alternative parameterizations here*). Let $Q_\\theta(s,a)$ be the output of the network, to estimate the optimal action value function in state $s$ and take action $a$ (and follow optimal policy thereafter).\n",
        "\n",
        "$$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R972TxQlgQX3"
      },
      "source": [
        "### Bellman optimality equation\n",
        "\n",
        "We will use Bellman optimality equation to find $\\theta$ such that the above approximation holds better. Recall that for optimal Q function $Q^\\ast(s,a)$ the following holds for all $(s,a)$\n",
        "\n",
        "$$Q^\\ast(s_t,a_t) = \\mathbb{E}\\big[r_t + \\gamma \\max_a Q^\\ast(s_{t+1},a)\\big]$$\n",
        "\n",
        "where the expectation is wrt the random reward $r_t$ and transition to the next state $s_{t+1}$. A natural objective to consider is\n",
        "\n",
        "$$\\min_\\theta\\  (Q_\\theta(s_t,a_t) -\\mathbb{E}\\big[r_t + \\gamma  \\max_a  Q_{\\hat \\theta}(s_{t+1},a)\\big])^2$$\n",
        "at the current or previous $\\hat \\theta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD-rkp3sgQX4"
      },
      "source": [
        "### Building the DQN model\n",
        "\n",
        "The first step is to build a neural network with parameters $\\theta$ that predicts $Q_\\theta(s,a)$ for any $(s,a)$. You can either build a network that\n",
        "\n",
        "* (in case of small number $K$ of discrete actions) takes as input a  representation of state $s$ and outputs a $K$-dimensional vector giving scores $Q(s,a), a=1,\\ldots, K$ for all actions\n",
        "\n",
        "or\n",
        "\n",
        "* takes as input a concatenated representation of state and action $(s,a)$ and output one dimensional score $Q_\\theta(s,a)$,\n",
        "\n",
        "Below we have provided a skeleton code (incomplete) for defining and training the Q-function. **You need to fill in the DNN model definition and loss function definition**. Refer to regression lab (lab 2) for help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LhmRGDjHgQX5"
      },
      "outputs": [],
      "source": [
        "# define neural net Q_\\theta(s,a) as a class\n",
        "\n",
        "class Qfunction(object):\n",
        "\n",
        "    def __init__(self, obssize, actsize, lr, device, loss='huber', opt='adam'):\n",
        "        \"\"\"\n",
        "        obssize: dimension of state space\n",
        "        actsize: dimension of action space\n",
        "        sess: sess to execute this Qfunction\n",
        "        optimizer:\n",
        "        \"\"\"\n",
        "        # DEFINE THE MODEL\n",
        "        self.model = torch.nn.Sequential(\n",
        "                    #TODO\n",
        "                    #input layer\n",
        "                    torch.nn.Linear(obssize, 512),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Linear(512, 256),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Linear(256, 128),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Linear(128, actsize)\n",
        "                ).to(device)\n",
        "\n",
        "        # DEFINE THE OPTIMIZER\n",
        "        if opt == 'adam':\n",
        "          self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        elif opt == 'rmsprop':\n",
        "          self.optimizer = torch.optim.RMSprop(self.model.parameters(), lr=lr, alpha=0.95, eps=0.01)\n",
        "        elif opt == 'adamw':\n",
        "          self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, amsgrad=True)\n",
        "\n",
        "        # RECORD HYPER-PARAMS\n",
        "        self.obssize = obssize\n",
        "        self.actsize = actsize\n",
        "        self.device = device\n",
        "        self.loss = loss\n",
        "\n",
        "    def _to_one_hot(self, y, num_classes):\n",
        "        \"\"\"\n",
        "        convert an integer vector y into one-hot representation\n",
        "        \"\"\"\n",
        "        scatter_dim = len(y.size())\n",
        "        y_tensor = y.view(*y.size(), -1)\n",
        "        zeros = torch.zeros(*y.size(), num_classes, dtype=y.dtype, device=self.device)\n",
        "        return zeros.scatter(scatter_dim, y_tensor, 1)\n",
        "\n",
        "\n",
        "    def compute_Qvalues(self, states, actions):\n",
        "        \"\"\"\n",
        "        input: list of numsamples state-action pairs\n",
        "        output: List of Q values for each input (s,a). The output will have size [numsamples, 1]\n",
        "        \"\"\"\n",
        "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
        "        #This will be different for neural network that takes as input a state-action pair\n",
        "\n",
        "        try:\n",
        "          states = torch.FloatTensor(states).to(self.device)\n",
        "        except TypeError:\n",
        "          pass\n",
        "        q_preds = self.model(states)\n",
        "        action_onehot = self._to_one_hot(actions.to(self.device), self.actsize)\n",
        "        q_preds_selected = torch.sum(q_preds * action_onehot, axis=-1)\n",
        "\n",
        "        return q_preds_selected\n",
        "\n",
        "    def compute_maxQvalues(self, states):\n",
        "        \"\"\"\n",
        "        input: a list of numsamples states\n",
        "        output: max_a Q(s,a) values for every input state s in states. The output will have size numsamples\n",
        "        \"\"\"\n",
        "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
        "        #if the neural takes as input a state-action pair, then the code will need to loop over all actions to compute all values\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        Qvalues = self.model(states)\n",
        "        q_preds_greedy = Qvalues.max(1).values\n",
        "\n",
        "        return q_preds_greedy\n",
        "\n",
        "    def compute_argmaxQ(self, state):\n",
        "        \"\"\"\n",
        "        input: one state s\n",
        "        output: arg max_a Q(self.model(states).cpu().data.numpy()s,a) values for the input state s. The output will have size 1\n",
        "        \"\"\"\n",
        "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
        "        #if the neural takes as input a state-action pair, then the code will need to loop over all actions to compute all values\n",
        "\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        Qvalue = self.model(state)\n",
        "        greedy_action = Qvalue.argmax().item()\n",
        "\n",
        "        return greedy_action\n",
        "\n",
        "    def take_action(self, state, possible_actions, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "          return np.random.choice(possible_actions)\n",
        "        else:\n",
        "          return self.compute_argmaxQ(state)\n",
        "\n",
        "\n",
        "    def train(self, states, actions, targets, verbose=False):\n",
        "        \"\"\"\n",
        "        states: numpy array as input to compute loss (s)\n",
        "        actions: numpy array as input to compute loss (a)\n",
        "        targets: numpy array as input to compute loss (Q targets)\n",
        "        \"\"\"\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(actions).long().to(self.device)\n",
        "        targets = torch.from_numpy(targets).float().to(self.device)\n",
        "\n",
        "        # COMPUTE Q PREDICTIONS for all state-action pairs\n",
        "        q_preds_selected = self.compute_Qvalues(states, actions)\n",
        "\n",
        "        # LOSS\n",
        "        if verbose: print(q_preds_selected.shape, targets.shape)\n",
        "        if self.loss == 'mse':\n",
        "          loss = torch.mean((q_preds_selected - targets)**2)\n",
        "        elif self.loss == 'huber':\n",
        "          loss_func = torch.nn.SmoothL1Loss()\n",
        "          loss = loss_func(q_preds_selected, targets)\n",
        "\n",
        "        # BACKWARD PASS\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # UPDATE\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.detach().cpu().data.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_wC8o5r5ws3H"
      },
      "outputs": [],
      "source": [
        "\n",
        "BASE_PATH = '/Users/amanchopra/Documents/School/MS/Fall 2023/RL/Final Project/Data/Processed'\n",
        "data = pd.read_csv(f\"{BASE_PATH}/transition_model_discrete_actions_90_min_history_rewards.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "jQlRoNm5w0N8",
        "outputId": "c423498f-5611-4624-9db0-44dd3720fc13"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BG_t0</th>\n",
              "      <th>BG_t1</th>\n",
              "      <th>BG_t2</th>\n",
              "      <th>BG_t3</th>\n",
              "      <th>BG_t4</th>\n",
              "      <th>BG_t5</th>\n",
              "      <th>BG_t6</th>\n",
              "      <th>BG_t7</th>\n",
              "      <th>BG_t8</th>\n",
              "      <th>BG_t9</th>\n",
              "      <th>...</th>\n",
              "      <th>IOB_t28</th>\n",
              "      <th>IOB_t29</th>\n",
              "      <th>IOB_t30</th>\n",
              "      <th>IOB_t31</th>\n",
              "      <th>IOB_t32</th>\n",
              "      <th>IOB_t33</th>\n",
              "      <th>IOB_t34</th>\n",
              "      <th>IOB_t35</th>\n",
              "      <th>simple_reward</th>\n",
              "      <th>magni_reward</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>127.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.491872</td>\n",
              "      <td>0.502033</td>\n",
              "      <td>0.510965</td>\n",
              "      <td>0.531246</td>\n",
              "      <td>0.543538</td>\n",
              "      <td>0.555829</td>\n",
              "      <td>0.568121</td>\n",
              "      <td>0.580412</td>\n",
              "      <td>0</td>\n",
              "      <td>-10.211281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>132.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.471303</td>\n",
              "      <td>0.491872</td>\n",
              "      <td>0.502033</td>\n",
              "      <td>0.510965</td>\n",
              "      <td>0.531246</td>\n",
              "      <td>0.543538</td>\n",
              "      <td>0.555829</td>\n",
              "      <td>0.568121</td>\n",
              "      <td>0</td>\n",
              "      <td>-12.579462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.469787</td>\n",
              "      <td>0.471303</td>\n",
              "      <td>0.491872</td>\n",
              "      <td>0.502033</td>\n",
              "      <td>0.510965</td>\n",
              "      <td>0.531246</td>\n",
              "      <td>0.543538</td>\n",
              "      <td>0.555829</td>\n",
              "      <td>0</td>\n",
              "      <td>-12.139707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>120.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.458479</td>\n",
              "      <td>0.469787</td>\n",
              "      <td>0.471303</td>\n",
              "      <td>0.491872</td>\n",
              "      <td>0.502033</td>\n",
              "      <td>0.510965</td>\n",
              "      <td>0.531246</td>\n",
              "      <td>0.543538</td>\n",
              "      <td>0</td>\n",
              "      <td>-12.800759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>106.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.444016</td>\n",
              "      <td>0.458479</td>\n",
              "      <td>0.469787</td>\n",
              "      <td>0.471303</td>\n",
              "      <td>0.491872</td>\n",
              "      <td>0.502033</td>\n",
              "      <td>0.510965</td>\n",
              "      <td>0.531246</td>\n",
              "      <td>0</td>\n",
              "      <td>-14.602987</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 111 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   BG_t0  BG_t1  BG_t2  BG_t3  BG_t4  BG_t5  BG_t6  BG_t7  BG_t8  BG_t9  ...  \\\n",
              "0  127.0  121.0  113.0   99.0   95.0   97.0  101.0  107.0  111.0  118.0  ...   \n",
              "1  132.0  127.0  121.0  113.0   99.0   95.0   97.0  101.0  107.0  111.0  ...   \n",
              "2  130.0  132.0  127.0  121.0  113.0   99.0   95.0   97.0  101.0  107.0  ...   \n",
              "3  120.0  130.0  132.0  127.0  121.0  113.0   99.0   95.0   97.0  101.0  ...   \n",
              "4  106.0  120.0  130.0  132.0  127.0  121.0  113.0   99.0   95.0   97.0  ...   \n",
              "\n",
              "    IOB_t28   IOB_t29   IOB_t30   IOB_t31   IOB_t32   IOB_t33   IOB_t34  \\\n",
              "0  0.491872  0.502033  0.510965  0.531246  0.543538  0.555829  0.568121   \n",
              "1  0.471303  0.491872  0.502033  0.510965  0.531246  0.543538  0.555829   \n",
              "2  0.469787  0.471303  0.491872  0.502033  0.510965  0.531246  0.543538   \n",
              "3  0.458479  0.469787  0.471303  0.491872  0.502033  0.510965  0.531246   \n",
              "4  0.444016  0.458479  0.469787  0.471303  0.491872  0.502033  0.510965   \n",
              "\n",
              "    IOB_t35  simple_reward  magni_reward  \n",
              "0  0.580412              0    -10.211281  \n",
              "1  0.568121              0    -12.579462  \n",
              "2  0.555829              0    -12.139707  \n",
              "3  0.543538              0    -12.800759  \n",
              "4  0.531246              0    -14.602987  \n",
              "\n",
              "[5 rows x 111 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qFfiHXqx_Hj",
        "outputId": "ce024a20-17c1-49a2-fa25-8e9945258633"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "simple_reward\n",
              " 0     23812\n",
              " 1     18604\n",
              "-10     3301\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['simple_reward'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    45717.000000\n",
              "mean        -8.245843\n",
              "std         11.819608\n",
              "min        -84.319007\n",
              "25%         -8.024595\n",
              "50%         -5.669213\n",
              "75%         -1.246025\n",
              "max         -0.000047\n",
              "Name: magni_reward, dtype: float64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['magni_reward'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC4rGthegQX9"
      },
      "source": [
        "At this point you can skip ahead to implementing the basic Q-learning that at every step $t$ in the environment\n",
        "* given state $s_t$, computes greedy actions from Q-values (using compute_argmaxQ function above) and uses $\\epsilon$-greedy select an action $a_t$,\n",
        "* makes observation of reward $r_t$ and next state $s_{t+1}$\n",
        "* using compute_maxQvalues() function, computes target\n",
        "  $$r_t + \\gamma \\max_a Q_\\theta(s_{t+1},a)$$\n",
        "and then retrains the Q-function using train() function above (with numsamples=1)\n",
        "\n",
        "However, for improved performance you may want to consider ideas like batch training (numsamples>1 is the batch size) with experience replay buffer and target-networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8RupUgMgQX-"
      },
      "source": [
        "**Replay Buffer**\n",
        "\n",
        "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$, when we minimize the Bellman error. When optimizing the Bellman error loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples from buffer $(s_i,a_i,r_i,s_{i}') \\sim R$ and then compute\n",
        "targets\n",
        "\n",
        "$$d_i=r_i + \\max_a \\gamma Q_{\\theta}(s_i^\\prime,a)$$\n",
        "for all $i$. Use the above training function train() with input as list $(s_i, a_i, d_i)_{i=1}^N$  to update parameters using backprop.\n",
        "\n",
        "**Target Network**\n",
        "\n",
        "Maintain a target network in addition to the original pricipal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta^-$ is copied from the principal network every $\\tau$ time steps. Target network is used to compute the targets for update\n",
        "\n",
        "$$d_i =  r_t + \\gamma \\max_a Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
        "\n",
        "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sIOVd8pgQX_"
      },
      "source": [
        "Hence several critical parts of the complete pseudocode for DQN is as follows:\n",
        "\n",
        "**Initialization.**\n",
        "principal network $Q_\\theta(s,a)$, target network $Q_{\\theta^{-}}(s,a)$. Replay buffer $R = \\{\\}$ (empty).\n",
        "\n",
        "**At each time step $t.$**\n",
        "The agent executes action using $\\epsilon-$greedy based on the principal network $Q_\\theta(s,a)$. To update $\\theta$: sample $N$ tuples $(s_i,a_i,r_i,s_i^\\prime) \\sim R$, compute empirical loss\n",
        "\n",
        "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - (r_i + \\gamma \\max_a Q_{\\theta^{-}}(s_i^\\prime,a))^2$$\n",
        "\n",
        "and update parameter $\\theta$ using backprop (just take one gradient step).\n",
        "\n",
        "**Update target network.**\n",
        "Every $\\tau$ time steps, update target network by copying $\\theta_{\\text{target}} \\leftarrow \\theta$.\n",
        "\n",
        "**Bellman target.**\n",
        "Above, we have defined the target values as being computed from a target net with parameter $\\theta^-$\n",
        "$$r_i + \\gamma \\max_a Q_{\\theta^{-}}(s_i^\\prime,a)$$\n",
        "It is worth thinking about what happens if we are at the end of an episode, that is, what if $s_i^\\prime$ here is a terminal state. In this case, should the Bellman error be defined exactly the same as above? Do we need some modifications? Think carefully about this as this will greatly impact the algorithmic performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6bTnNEpgQX_"
      },
      "source": [
        "### Implementation of replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJnxONjRgQYA",
        "outputId": "044012f9-30e6-486b-9c8f-89856230ebc2"
      },
      "outputs": [],
      "source": [
        "# Implement replay buffer\n",
        "import random\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    def __init__(self, maxlength):\n",
        "        \"\"\"\n",
        "        maxlength: max number of tuples to store in the buffer\n",
        "        if there are more tuples than maxlength, pop out the oldest tuples\n",
        "        \"\"\"\n",
        "        self.buffer = deque()\n",
        "        self.number = 0\n",
        "        self.maxlength = maxlength\n",
        "\n",
        "    def append(self, experience):\n",
        "        \"\"\"\n",
        "        this function implements appending new experience tuple\n",
        "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
        "        \"\"\"\n",
        "        self.buffer.append(experience)\n",
        "        self.number += 1\n",
        "\n",
        "    def pop(self):\n",
        "        \"\"\"\n",
        "        pop out the oldest tuples if self.number > self.maxlength\n",
        "        \"\"\"\n",
        "        while self.number > self.maxlength:\n",
        "            self.buffer.popleft()\n",
        "            self.number -= 1\n",
        "\n",
        "    def sample(self, batchsize):\n",
        "        \"\"\"\n",
        "        this function samples 'batchsize' experience tuples\n",
        "        batchsize: size of the minibatch to be sampled\n",
        "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
        "        \"\"\"\n",
        "        minibatch = random.sample(self.buffer,batchsize)\n",
        "        return minibatch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXJfV58XgQYA"
      },
      "source": [
        "### Code snippet for copying target network\n",
        "You may use th following to update target network i.e. to copy from principal network to target network. We need to use tensorflow scope to distinguish the computational graphs of target and principal networks. The following function builds a tensorflow operation that does the copying $\\theta^- \\leftarrow \\theta$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sn2ptEZwgQYB"
      },
      "outputs": [],
      "source": [
        "def run_target_update(Qprincipal, Qtarget):\n",
        "    for v,v_ in zip(Qprincipal.model.parameters(), Qtarget.model.parameters()):\n",
        "        v_.data.copy_(v.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    45717.000000\n",
              "mean         0.039898\n",
              "std          0.341363\n",
              "min          0.000000\n",
              "25%          0.000000\n",
              "50%          0.000000\n",
              "75%          0.000000\n",
              "max         11.000000\n",
              "Name: InsulinDelivered, dtype: float64"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['InsulinDelivered'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wgIE5aJgQYB"
      },
      "source": [
        "## Main code for DQN\n",
        "Now that we have all the ingredients for DQN, we can write the main procedure to train DQN on a given environment. The implementation is straightforward if you follow the pseudocode pdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QuNNnNlgxVjv"
      },
      "outputs": [],
      "source": [
        "# helper function to get index of state from data\n",
        "def get_state_ind(df, state, s_cols):\n",
        "        temp = df[pd.concat([df[s_col] == v for s_col, v in zip(s_cols, state)], axis=1).all(axis=1)]\n",
        "        if temp.shape[0] > 0:\n",
        "            return temp.sample(1).index[0]\n",
        "        return -1\n",
        "\n",
        "class CustomEnv(gym.Env):\n",
        "    def __init__(self, dataframe, reward='simple_reward'):\n",
        "        super(CustomEnv, self).__init__()\n",
        "        self.df = dataframe\n",
        "        \n",
        "        feature_dim = int((max([int(col[4:]) for col in dataframe.columns if 'BG' in col]) + 1) / 2)\n",
        "\n",
        "        self.s_cols = [f'BG_t{i}' for i in range(feature_dim)] + [f'HR_t{i}' for i in range(feature_dim)] + [f'IOB_t{i}' for i in range(feature_dim)]\n",
        "        self.a_col = 'InsulinDelivered'\n",
        "        self.s_prime_cols = [f'BG_t{i}' for i in range(feature_dim, 2*feature_dim)] + [f'HR_t{i}' for i in range(feature_dim, 2*feature_dim)] + [f'IOB_t{i}' for i in range(feature_dim, 2*feature_dim)]\n",
        "        self.reward_col = reward\n",
        "\n",
        "        actions = dataframe[self.a_col]\n",
        "        self.action_space = spaces.Discrete(n=actions.nunique())\n",
        "\n",
        "        bg_min = min([data[f'BG_t{i}'].min() for i in range(feature_dim*2)])\n",
        "        bg_max = max([data[f'BG_t{i}'].max() for i in range(feature_dim*2)])\n",
        "        hr_min = min([data[f'HR_t{i}'].min() for i in range(feature_dim*2)])\n",
        "        hr_max = max([data[f'HR_t{i}'].max() for i in range(feature_dim*2)])\n",
        "        iob_min = min([data[f'IOB_t{i}'].min() for i in range(feature_dim*2)])\n",
        "        iob_max = max([data[f'IOB_t{i}'].max() for i in range(feature_dim*2)])\n",
        "        \n",
        "        self.observation_space = spaces.Box(low=np.array([bg_min]*feature_dim + [hr_min]*feature_dim + [iob_min]*feature_dim), high=np.array([bg_max]*feature_dim + [hr_max]*feature_dim + [iob_max]*feature_dim))\n",
        "        self.current_state_ind = None\n",
        "\n",
        "        \n",
        "    def reset(self, seed=None):\n",
        "        if seed:\n",
        "            sample = self.df.sample(1, random_state=seed)\n",
        "            s_ind = sample.index[0]\n",
        "            s = sample[self.s_cols].values.flatten()\n",
        "        else:\n",
        "            sample = self.df.sample(1)\n",
        "            s_ind = sample.index[0]\n",
        "            s = sample[self.s_cols].values.flatten()\n",
        "        self.current_state_ind = s_ind\n",
        "        return s\n",
        "\n",
        "    def step(self, action):\n",
        "        # get the next state and reward based on the current state and action\n",
        "        next_state_tx = self.df.iloc[self.current_state_ind]\n",
        "        if next_state_tx[self.a_col] != action:\n",
        "            raise Exception(f\"Cannot take action (InsulinDelivered={action}) as this action doesn't exist for the current state at index {self.current_state_ind}!\")\n",
        "        next_state  = next_state_tx[self.s_prime_cols].values\n",
        "        reward = next_state_tx[self.reward_col]\n",
        "        done = self.is_done()\n",
        "        info = {}\n",
        "\n",
        "        self.current_state_ind = get_state_ind(self.df, next_state, self.s_cols)\n",
        "\n",
        "        if self.current_state_ind == -1: # get closest state to next_state and update next_state\n",
        "            self.current_state_ind = self.get_closest_state_ind(next_state)\n",
        "            next_state = self.df.iloc[self.current_state_ind][self.s_cols].values\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "    \n",
        "    def does_action_exist(self, action):\n",
        "        if action in self.df[self.a_col]:\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def get_closest_state_ind(self, state):\n",
        "        distances = np.linalg.norm(state - self.df[self.s_cols].values, axis=1)\n",
        "        return np.argsort(distances)[0]\n",
        "\n",
        "    \n",
        "    def get_closest_action(self, action):\n",
        "        ind = np.random.choice(np.where((self.df[self.a_col] - action).abs() == (self.df[self.a_col] - action).abs().min())[0])\n",
        "        return data.iloc[ind][self.a_col]\n",
        "\n",
        "    def is_done(self):\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_action(df, state, env):\n",
        "    state_ind = get_state_ind(df, state, env.s_cols)\n",
        "    return df.iloc[state_ind][env.a_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "i3zUTBupxbih"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/rl/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        }
      ],
      "source": [
        "env = CustomEnv(data)\n",
        "observation = env.reset()\n",
        "#print(observation)\n",
        "# action = env.action_space.sample()\n",
        "action = get_action(data, observation, env)\n",
        "next_observation, reward, done, info = env.step(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "45219cb311904b08aa78d0c5765e99c5",
            "7b0a959e6a774a72ad2e7539d9bca64a",
            "290e12287bbe4f22a3798e791169b274",
            "d49640d6db1b467badf1774f074c8bd5",
            "b8eeb5b7f19f4ff49808a41c3b268f38",
            "190a0163792e4038a9d778b7a943b6aa",
            "f20f21a89c604d239fd23f3420c9e77e",
            "bd96d67c8f304cd3991b3dc0ada24776"
          ]
        },
        "id": "LHlgGA9_gQYB",
        "outputId": "4b5b2662-f946-4fe3-ce77-175b4f7b66c2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe src='https://wandb.ai/amanichopra/rl-based-artificial-pancreas/runs/vogiyc12?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
            ],
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7fe99fb27cd0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:vogiyc12) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train reward moving average</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training reward</td><td>█▆█▅▅▇▆▄▁▅▆▄▆▅▅▅▃▄▄▂▆▄▆▆▅▆▆▅▆▃▆█▃▅▂▄▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train reward moving average</td><td>0</td></tr><tr><td>training reward</td><td>12.0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">earnest-bird-4</strong> at: <a href='https://wandb.ai/amanichopra/rl-based-artificial-pancreas/runs/vogiyc12' target=\"_blank\">https://wandb.ai/amanichopra/rl-based-artificial-pancreas/runs/vogiyc12</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20231204_214722-vogiyc12/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:vogiyc12). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/amanchopra/Documents/School/MS/Fall 2023/RL/Final Project/Code/wandb/run-20231204_215205-02jkz9f9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/amanichopra/rl-based-artificial-pancreas' target=\"_blank\">https://wandb.ai/amanichopra/rl-based-artificial-pancreas</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/amanichopra/rl-based-artificial-pancreas/runs/02jkz9f9' target=\"_blank\">https://wandb.ai/amanichopra/rl-based-artificial-pancreas/runs/02jkz9f9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/rl/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "-1.0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/Users/amanchopra/Documents/School/MS/Fall 2023/RL/Final Project/Code/DQN.ipynb Cell 23\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X26sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m#greedy choice below. Use epsilon greedy for exploration\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X26sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m#action = Qprincipal.take_action(np.expand_dims(obs,0), [i for i in range(env.action_space.n)], epsilon)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X26sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m action \u001b[39m=\u001b[39m get_action(data, obs, env)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X26sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m eps_actions[action] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X26sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m newobs, r, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/amanchopra/Documents/School/MS/Fall%202023/RL/Final%20Project/Code/DQN.ipynb#X26sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m done_ \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m done \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
            "\u001b[0;31mKeyError\u001b[0m: -1.0"
          ]
        }
      ],
      "source": [
        "%%wandb\n",
        "#remove above line if you do not want to see inline plots from wandb\n",
        "\n",
        "# hyper-parameters\n",
        "lr = 1e-3  # learning rate for gradient update\n",
        "batchsize = 64  # batchsize for buffer sampling\n",
        "maxlength = 1000  # max number of tuples held by buffer\n",
        "tau = 100  # time steps for target update\n",
        "episodes = 300  # number of episodes to run\n",
        "episode_max_len = 100 # length of episode\n",
        "initialsize = 500  # initial time steps before start training\n",
        "epsilon = .2  # constant for exploration\n",
        "gamma = .99  # discount\n",
        "q_update_num_steps = 1 # time steps for q network update\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "verbose = True\n",
        "opt = 'adamw'\n",
        "loss = 'huber'\n",
        "\n",
        "run = wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"rl-based-artificial-pancreas\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"method\": \"dqn\",\n",
        "    \"lr\": lr,\n",
        "    \"buffer_batchsize\": batchsize,\n",
        "    \"buffer_maxlength\": maxlength,\n",
        "    \"tau\": tau,\n",
        "    \"episodes\": episodes,\n",
        "    \"initial_size\": initialsize,\n",
        "    \"epsilon\": epsilon,\n",
        "    \"gamma\": gamma,\n",
        "    \"q_update_num_steps\": q_update_num_steps,\n",
        "    \"device\": device,\n",
        "    \"optimizer\": opt,\n",
        "    \"loss\": loss,\n",
        "    \"max_episode_len\": episode_max_len\n",
        "    }\n",
        ")\n",
        "\n",
        "# initialize environment\n",
        "# env = gym.make(envname)\n",
        "env = CustomEnv(data)\n",
        "obssize = env.observation_space.low.size\n",
        "actsize = env.action_space.n\n",
        "\n",
        "# initialize Q-function networks (princpal and target)\n",
        "Qprincipal = Qfunction(obssize, actsize, lr, device, opt=opt, loss=loss)\n",
        "Qtarget = Qfunction(obssize, actsize, lr, device, opt=opt, loss=loss)\n",
        "\n",
        "# initialization of graph and buffer\n",
        "buffer = ReplayBuffer(maxlength)\n",
        "\n",
        "# main iteration\n",
        "rrecord = []\n",
        "totalstep = 0\n",
        "\n",
        "for episode in range(episodes):\n",
        "\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    rsum = 0\n",
        "    eps_actions = {i: 0 for i in range(env.action_space.n)}\n",
        "\n",
        "    for i in range(episode_max_len):\n",
        "        if done: break\n",
        "\n",
        "        #greedy choice below. Use epsilon greedy for exploration\n",
        "        #action = Qprincipal.take_action(np.expand_dims(obs,0), [i for i in range(env.action_space.n)], epsilon)\n",
        "        action = get_action(data, obs, env)\n",
        "\n",
        "        eps_actions[action] += 1\n",
        "\n",
        "        newobs, r, done, _ = env.step(action)\n",
        "        done_ = 1 if done else 0\n",
        "        e = (obs, action, r, done_, newobs)\n",
        "\n",
        "        #IF NOT USING BUFFER:\n",
        "        #use single sample (obs, action, r, done_, newobs) with Qtarget to compute target and train Qprincipal\n",
        "\n",
        "        # ELSE IF USING REPLAY BUFFER\n",
        "        # append experiences e to buffer\n",
        "\n",
        "        buffer.append(e)\n",
        "        buffer.pop()\n",
        "\n",
        "        #every few episodes (decide the frequency) sample a minibatch from buffer\n",
        "        #compute targets in batch using Qtarget and train  Qprincipal\n",
        "        if totalstep % q_update_num_steps == 0 and buffer.number >= batchsize:\n",
        "          samples = buffer.sample(batchsize)\n",
        "          states = [e[0] for e in samples]\n",
        "          actions = [e[1] for e in samples]\n",
        "          rewards = [e[2] for e in samples]\n",
        "          dones = np.array([e[3] for e in samples], dtype=bool)\n",
        "          next_states = [e[4] for e in samples]\n",
        "\n",
        "          non_ternimal_next_states = [next_state for next_state, done in zip(next_states, dones) if not done]\n",
        "          non_ternimal_next_states_values = Qtarget.compute_maxQvalues(non_ternimal_next_states)\n",
        "\n",
        "          next_states_values = np.zeros(batchsize)\n",
        "          next_states_values[~dones] = non_ternimal_next_states_values.cpu().data.numpy()\n",
        "\n",
        "          #next_states_values = [0 if dones[i] else next(non_ternimal_next_states_values) for i in range(batchsize)]\n",
        "\n",
        "          targets = [rewards[i] if dones[i] else rewards[i] + gamma * next_states_values[i] for i in range(batchsize)]\n",
        "          Qprincipal.train(np.array(states), np.array(actions), np.array(targets), verbose=False)\n",
        "\n",
        "\n",
        "        #UPDATE target network\n",
        "        #every tau steps update copy the principal network to the target network\n",
        "        if totalstep % tau == 0:\n",
        "            run_target_update(Qprincipal, Qtarget)\n",
        "\n",
        "        # update\n",
        "        totalstep += 1\n",
        "        rsum += r\n",
        "        obs = newobs\n",
        "\n",
        "    rrecord.append(rsum)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # printing functions for debugging purposes. Feel free to add more\n",
        "    if verbose and episode % 10 == 0:\n",
        "       print('buffersize {}'.format(buffer.number))\n",
        "       print('episode {} ave training returns {}'.format(episode, np.mean(rrecord[-10:])))\n",
        "\n",
        "    #printing moving averages for smoothed visualization.\n",
        "    fixedWindow=100\n",
        "    movingAverage=0\n",
        "    if len(rrecord) >= fixedWindow:\n",
        "        movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
        "\n",
        "    wandb.log({ \"training reward\" : rsum, \"train reward moving average\" : movingAverage})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGtiM-vmgQYC"
      },
      "source": [
        "Finally, we evaluate the performance of the trained agent. We will evaluate the performance of the greedy policy wrt learned Q-function. The evaluation will be run 10 times, each for eval_epsiodes and print out the average performance across these episodes. Please **do not** change the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "L55512RpgQYC"
      },
      "outputs": [],
      "source": [
        "### DO NOT CHANGE\n",
        "def evaluate(Q, env, episodes):\n",
        "    # main iteration\n",
        "    score = 0.0\n",
        "    for episode in range(episodes):\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        rsum = 0\n",
        "\n",
        "        while not done:\n",
        "            # always greedy\n",
        "            action = Q.compute_argmaxQ(np.expand_dims(obs,0))\n",
        "\n",
        "\n",
        "            # mdp stepping forward\n",
        "            newobs, r, done, _ = env.step(action)\n",
        "\n",
        "            # update data\n",
        "            rsum += r\n",
        "            obs = newobs\n",
        "\n",
        "\n",
        "        wandb.log({\"eval reward\" : rsum})\n",
        "        score = score + rsum\n",
        "    score = score/episodes\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9-OyN_1gQYD",
        "outputId": "4f904521-41db-4cab-9f0a-19e8b9b76141"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval performance of DQN agent: 0.0\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE CODE HERE\n",
        "# after training, we will evaluate the performance of the agent\n",
        "# on a target environment\n",
        "# env_test = gym.make(envname)\n",
        "env_test = CustomEnv(data)\n",
        "eval_episodes = 1000\n",
        "score = evaluate(Qprincipal, env_test, eval_episodes)\n",
        "wandb.run.summary[\"score\"]=score\n",
        "\n",
        "print(\"eval performance of DQN agent: {}\".format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574,
          "referenced_widgets": [
            "eb7e5dd479084da08180b70fba0d3e44",
            "1d6754381337431195a2fafe3efa9483",
            "adabf5316aef42cd95cf88553fd42656",
            "a27b3a580aca4773b0b7fdc27af55df8",
            "0045943bff2e4aa9b1d168b1661e4f64",
            "9b184009643841869af4093d187c8529",
            "1e458618cbb847ceb470a4383c117adc",
            "aa85385400da44f588ae4b52c9bdb15c"
          ]
        },
        "id": "VBt7yCCygQYD",
        "outputId": "b481f122-855e-490e-fe58-4dafbcbb24b5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb7e5dd479084da08180b70fba0d3e44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train reward moving average</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval reward</td><td>0</td></tr><tr><td>score</td><td>0.0</td></tr><tr><td>train reward moving average</td><td>0.0</td></tr><tr><td>training reward</td><td>0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">noble-smoke-2</strong> at: <a href='https://wandb.ai/rl23/rl-cartpole/runs/k2uep8fk' target=\"_blank\">https://wandb.ai/rl23/rl-cartpole/runs/k2uep8fk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20231203_174238-k2uep8fk/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0045943bff2e4aa9b1d168b1661e4f64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "190a0163792e4038a9d778b7a943b6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d6754381337431195a2fafe3efa9483": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0045943bff2e4aa9b1d168b1661e4f64",
            "placeholder": "​",
            "style": "IPY_MODEL_9b184009643841869af4093d187c8529",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "1e458618cbb847ceb470a4383c117adc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "290e12287bbe4f22a3798e791169b274": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f20f21a89c604d239fd23f3420c9e77e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd96d67c8f304cd3991b3dc0ada24776",
            "value": 1
          }
        },
        "45219cb311904b08aa78d0c5765e99c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b0a959e6a774a72ad2e7539d9bca64a",
              "IPY_MODEL_290e12287bbe4f22a3798e791169b274"
            ],
            "layout": "IPY_MODEL_d49640d6db1b467badf1774f074c8bd5"
          }
        },
        "7b0a959e6a774a72ad2e7539d9bca64a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8eeb5b7f19f4ff49808a41c3b268f38",
            "placeholder": "​",
            "style": "IPY_MODEL_190a0163792e4038a9d778b7a943b6aa",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "9b184009643841869af4093d187c8529": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a27b3a580aca4773b0b7fdc27af55df8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa85385400da44f588ae4b52c9bdb15c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "adabf5316aef42cd95cf88553fd42656": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e458618cbb847ceb470a4383c117adc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa85385400da44f588ae4b52c9bdb15c",
            "value": 1
          }
        },
        "b8eeb5b7f19f4ff49808a41c3b268f38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd96d67c8f304cd3991b3dc0ada24776": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d49640d6db1b467badf1774f074c8bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7e5dd479084da08180b70fba0d3e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d6754381337431195a2fafe3efa9483",
              "IPY_MODEL_adabf5316aef42cd95cf88553fd42656"
            ],
            "layout": "IPY_MODEL_a27b3a580aca4773b0b7fdc27af55df8"
          }
        },
        "f20f21a89c604d239fd23f3420c9e77e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
