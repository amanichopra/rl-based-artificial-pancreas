{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vy6LMgCzH3rR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41vSrepq_tAI",
        "outputId": "ded4b47b-09d2-4b41-9018-5f9313f307cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnsjKu9rH3rV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0c475a-f71f-4645-ba7d-7cc6406e7610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamanichopra\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds_iCb03H3rV"
      },
      "source": [
        "## Policy Gradient\n",
        "\n",
        "In the previous lab, we talked about value based method for reinforcement learning. In this lab, we focus on policy based method.\n",
        "\n",
        "In policy based methods, intead of defining a value function $Q_\\theta(s,a)$ and inducing a policy based on argmax, we parameterize a stochastic policy directly. The policy is parameterized as a categorical distribution over actions. Let it be $\\pi_\\phi(s)$ with parameter $\\phi$, then the policy is defined by sampling actions $$a \\sim \\pi_\\phi(s)$$\n",
        "\n",
        "The policy induces a probability $p(\\tau)$ over trajectories $\\tau = \\{s_0,a_0,s_1,a_1,..\\}$. The expected total discounted reward is\n",
        "\n",
        "$$\\rho(\\phi) = \\mathbb{E}_{\\tau \\sim p(\\tau)} \\big[R(\\tau)\\big] = \\mathbb{E}_{\\pi_\\phi} \\big[\\sum_{t=0}^\\infty r_t \\gamma^t \\big]$$\n",
        "\n",
        "The aim is to find $\\phi$ such that the expected reward induced by $\\pi_\\phi$ is maximized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVr1KT73H3rY"
      },
      "source": [
        "### Policy Gradient Computation\n",
        "\n",
        "We can derive policy gradient\n",
        "\n",
        "$$\\nabla_\\phi \\rho(\\phi) = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty Q^{\\pi_\\phi}(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(s_t, a_t) \\big]$$\n",
        "\n",
        "To compute the gradient for update $\\phi \\leftarrow \\phi + \\alpha \\nabla_\\phi \\rho(\\phi)$, we need to estimate $Q^{\\pi_\\phi}(s,a)$. Since $Q^{\\pi_\\phi}(s,a)$ is usually not analytically accessible, it can be approximated by\n",
        "1. Monte Carlo estimate\n",
        "2. Train a value function $Q_\\theta(s,a) \\approx Q^{\\pi_\\phi}(s,a)$ and use it as a proxy\n",
        "3. Mixture of both above\n",
        "\n",
        "Before estimating $Q^{\\pi_\\phi}(s,a)$, let us write a parameterized policy over actions. The policy $\\pi_\\phi(s)$ takes a state as input and outputs a categorical distribution over actions. For example, if we have two actions, the probability vector to output is of the form $\\pi_\\phi(s)=[0.6,0.4]$.\n",
        "\n",
        "**Loss function**\n",
        "Given samples of state action pairs $(s_i, a_i)$ and estimate $Q_i$ for $Q^{\\pi_\\phi}(s_i, a_i)$, for $i=1,\\ldots, $ we  set the loss function  as\n",
        "$$-\\frac{1}{N} \\sum_{i=1}^N Q_i \\log(\\pi_\\phi(s_i, a_i)) $$\n",
        "The loss function enables us  to compute policy gradients in implementation. The negative gradient of the above loss function has the form\n",
        "\n",
        "$$\\frac{1}{N} \\sum_{i=1}^N Q_i  \\nabla_\\phi \\log \\pi_\\phi(s_i, a_i) $$\n",
        "\n",
        "where $Q_i$s are estimated and $\\nabla_\\phi \\log\\pi_\\phi(s_i, a_i)$s are computed via backprop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eociUA9MH3rZ"
      },
      "outputs": [],
      "source": [
        "class PNet(torch.nn.Module):\n",
        "    def __init__(self, observation_space, action_space):\n",
        "        super(PNet, self).__init__()\n",
        "        self.l1 = torch.nn.Linear(observation_space, 128)\n",
        "        self.l2 = torch.nn.Linear(128, action_space)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        actions = self.l2(x)\n",
        "        action_probs = torch.nn.functional.softmax(actions, dim=1)\n",
        "        return action_probs\n",
        "\n",
        "# define neural net \\pi_\\phi(s) as a class\n",
        "class Policy(object):\n",
        "\n",
        "    def __init__(self, obssize, actsize, lr, device):\n",
        "        \"\"\"\n",
        "        obssize: size of the states\n",
        "        actsize: size of the actions\n",
        "        \"\"\"\n",
        "        # TODO DEFINE THE MODEL\n",
        "        self.model = PNet(obssize, actsize).to(device)\n",
        "\n",
        "        # DEFINE THE OPTIMIZER\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # RECORD HYPER-PARAMS\n",
        "        self.obssize = obssize\n",
        "        self.actsize = actsize\n",
        "        self.device = device\n",
        "\n",
        "        # TEST\n",
        "        #self.compute_prob(np.random.randn(obssize).reshape(1, -1))\n",
        "\n",
        "    def compute_prob(self, states):\n",
        "        \"\"\"\n",
        "        compute prob distribution over all actions given state: pi(s)\n",
        "        states: numpy array of size [numsamples, obssize]\n",
        "        return: numpy array of size [numsamples, actsize]\n",
        "        \"\"\"\n",
        "        states = torch.from_numpy(states).float().unsqueeze(0).to(self.device)\n",
        "        return self.model(states)\n",
        "\n",
        "    def _to_one_hot(self, y, num_classes):\n",
        "        \"\"\"\n",
        "        convert an integer vector y into one-hot representation\n",
        "        \"\"\"\n",
        "        scatter_dim = len(y.size())\n",
        "        y_tensor = y.view(*y.size(), -1)\n",
        "        zeros = torch.zeros(*y.size(), num_classes, dtype=y.dtype).to(self.device)\n",
        "        return zeros.scatter(scatter_dim, y_tensor, 1)\n",
        "\n",
        "\n",
        "    def train(self, ads, lps):\n",
        "        policy_loss = []\n",
        "\n",
        "        for d, lp in zip(ads, lps):\n",
        "            # gradient ascent\n",
        "            policy_loss.append(-d * lp)\n",
        "        self.optimizer.zero_grad()\n",
        "        sum(policy_loss).backward()\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBjSdu--H3rb"
      },
      "source": [
        "Try to rollout trajecories using the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAhoFFK5H3rc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adfd22ce-cee2-4d48-8832-dda5ef12aea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# Below is a set of template code for running a policy to interact with the environment\n",
        "# It initializes a policy and runs it\n",
        "# Note that you may not be able to run the code properly if there still some undefined components on the Policy class\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "obssize = env.observation_space.low.size\n",
        "actsize = env.action_space.n\n",
        "policyinit=Policy(obssize, actsize, 0.1, device)\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    prob = policyinit.compute_prob(obs).cpu().data.numpy().ravel()\n",
        "    prob /= np.sum(prob) #normalizing again to account for numerical errors\n",
        "    action = np.random.choice(actsize, p=prob.flatten(), size=1)[0] #choose according distribution prob\n",
        "    obs, reward, done, info = env.step(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANyI3JHrH3rc"
      },
      "source": [
        "### Estimate $Q^\\pi(s,a)$\n",
        "\n",
        "To estimate $Q^\\pi(s,a)$, we can rollout the policy until the episode ends and do monte carlo estimate. In particular, under policy $\\pi$, we start from state action $(s_0,a_0)$ and rollout the policy to generate a trajectory $\\{s_0,a_0,s_1,a_1...s_T,a_T\\}$, with corresponding reward $r_0,r_1...r_T$. Monte carlo estimate is\n",
        "\n",
        "$$\\hat{Q}_{MC}(s,a) = \\sum_{t=0}^T r_t \\gamma^t \\approx Q^\\pi(s,a)$$\n",
        "\n",
        "This estimate by itself is of high variance. Using pure monte carlo estimate may work but the gradient can have large variance and hence take the algorithm  a long time to converge. We can reduce variance using baseline. Recall the derivation of PG\n",
        "\n",
        "$$\\nabla_\\phi \\rho(\\phi) = \\mathbb{E}_{\\pi_\\phi} \\big[\\sum_{t=0}^\\infty Q^{\\pi_\\phi}(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(s_t, a_t) \\big] = \\mathbb{E}_{\\pi_\\phi} \\big[\\sum_{t=0}^\\infty ( Q^{\\pi_\\phi}(s_t,a_t) - b(s_t)) \\nabla_\\phi \\log \\pi_\\phi(s_t, a_t) \\big]$$\n",
        "\n",
        "where $b(s_t)$ can be any function of state $s_t$. $b(s_t)$ is called baseline. Optimal baseline function is hard to compute, but a good proxy is the value function $V^{\\pi_\\phi}(s_t)$. Hence the gradient has the form\n",
        "$$\\nabla_\\phi \\rho(\\pi_{\\phi}) = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty A^{\\pi_\\phi}(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(s_t, a_t) \\big]$$\n",
        "\n",
        "where $A^{\\pi_\\phi}(s,a)$ is the advantage. Hence we can train a value function $V^{\\pi_\\phi}(s)$ along side the policy and use it as baseline to reduce the variance of PG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1iC_TCiH3rc"
      },
      "source": [
        "Hence we also parameterize a value function $V_\\theta(s) \\approx V^{\\pi_\\phi}(s)$ with parameter $\\theta$ to serve as baseline. The function takes as input the states $s$ and outputs a real value.\n",
        "\n",
        "Notice that unlike DQN, where $Q_\\theta(s,a) \\approx Q^\\ast(s,a)$, now we have $V_\\theta(s) \\approx V^{\\pi_\\phi}(s)$. Therefore, we have a moving target to approximate, that changes with the current policy $\\pi_\\phi$. As $\\phi$ is updated by PG, $\\pi_\\phi$ keeps changing, which changes $V^{\\pi_\\phi}(s)$ as well. We need to adapt $V_\\theta(s)$ online to cater for the change in policy.\n",
        "\n",
        "Recall that to evaluate a policy $\\pi$, we collect rollouts using $\\pi$. If we start with state $s_0$, the reward following $\\pi$ thereafter is $r_0...r_{T}$  then\n",
        "\n",
        "$$V^\\pi(s_0) \\approx \\sum_{t=0}^{T} r_t \\gamma^{t} = \\hat{V}(s_0)$$\n",
        "\n",
        "In general, given a trajectory $(s_0, a_0, s_1, a_1, r_1, s_2, a_2, r_2, ..., s_{T+1})$\n",
        "\n",
        "$$\\hat{V}(s_i) = \\sum_{i=t}^{T} r_i \\gamma^{i-t}$$\n",
        "\n",
        "And the objective to minimize over is\n",
        "$$\\frac{1}{T+1} \\sum_{i=0}^{T} (V_\\theta(s_i) - \\hat{V}(s_i))^2$$\n",
        "\n",
        "Since the policy keeps updating, we do not need to minimize the above objective to optimality. In practice, taking one gradient step w.r.t. above objective suffices.\n",
        "\n",
        "In the code cell below, define the neural network to learn value function estimate. The implementation is similar to Qfunction class in lab3, except that inputs are only states, and not actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttwLR1wmH3rd"
      },
      "outputs": [],
      "source": [
        "class VNet(torch.nn.Module):\n",
        "    def __init__(self, observation_space):\n",
        "        super(VNet, self).__init__()\n",
        "        self.l1 = torch.nn.Linear(observation_space, 128)\n",
        "        self.l2 = torch.nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        state_value = self.l2(x)\n",
        "        return state_value\n",
        "\n",
        "# TODO: define value function as a class. You need to define the model and set the loss.\n",
        "\n",
        "class ValueFunction(object):\n",
        "    def __init__(self, obssize, lr, device):\n",
        "        \"\"\"\n",
        "        obssize: size of states\n",
        "        \"\"\"\n",
        "        # TODO DEFINE THE MODEL\n",
        "        self.model = VNet(obssize).to(device)\n",
        "\n",
        "        # DEFINE THE OPTIMIZER\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # RECORD HYPER-PARAMS\n",
        "        self.obssize = obssize\n",
        "        self.device = device\n",
        "\n",
        "        # TEST\n",
        "        #self.compute_values(np.random.randn(obssize).reshape(1, -1))\n",
        "\n",
        "    def compute_values(self, states):\n",
        "        \"\"\"\n",
        "        compute value function for given states\n",
        "        states: numpy array of size [numsamples, obssize]\n",
        "        return: numpy array of size [numsamples]\n",
        "        \"\"\"\n",
        "        if not torch.is_tensor(states): states = torch.FloatTensor(states).to(self.device)\n",
        "        return self.model(states)\n",
        "\n",
        "    def train(self, G, state_vals):\n",
        "        \"\"\"\n",
        "        states: numpy array\n",
        "        targets: numpy array\n",
        "        \"\"\"\n",
        "\n",
        "        #calculate MSE loss\n",
        "        val_loss = torch.nn.functional.mse_loss(state_vals, G)\n",
        "\n",
        "        #Backpropagate\n",
        "        self.optimizer.zero_grad()\n",
        "        val_loss.backward()\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtFDyO_sH3re"
      },
      "source": [
        "### Summary of pseudocode\n",
        "\n",
        "The critical components of the pseudocode are as follows.\n",
        "\n",
        "**Collect trajectories** Given current policy $\\pi_\\phi$, we can rollout using the policy by executing $a_t \\sim \\pi_\\phi(s_t)$.\n",
        "\n",
        "**Update value function** Value function update is based on minimizing the L2 loss between predicted value function and estimated value functions. For each state $s_i, i=0,\\ldots, T$ in a trajectory of length $T+1$, compute $\\hat{V}(s_i)$ as dicounted reward over the rest of the path (as defined above).  \n",
        "Then take one gradient step to update $\\theta$ using the gradient of the following loss:\n",
        "\n",
        "$$\\frac{1}{T+1} \\sum_{i=0}^T(V_\\theta(s_i) - \\hat{V}(s_i))^2$$\n",
        "\n",
        "For your conveience, below we have provided a function discounted_rewards(r,gamma) that takes as inputs a list of $T$ rewards $r$ and computes all discounted rewards $\\hat V(s_i), i=0, 1, 2, \\ldots, T$.\n",
        "\n",
        "**Update policy using PG** To compute PG, we need to first monte carlo estimate action-value function $\\hat{Q}(s_i,a_i)$. Given a trajectory with rewards $r=[r_0, r_1, r_2, \\ldots, r_T]$, this can also be computed for all $s_i, a_i$ in this trajectory using the discounted_rewards(r, gamma) function below.\n",
        "\n",
        "Then use value function as a baseline to compute advantage\n",
        "\n",
        "$$\\hat{A}(s_i,a_i) = \\hat{Q}(s_i,a_i) - V_\\theta(s_i)$$\n",
        "\n",
        "Then compute surrogate loss\n",
        "\n",
        "$$L = - \\frac{1}{(T+1)}\\sum_{i} \\hat{A}(s_i,a_i) \\log \\pi(a_i|s_i) $$\n",
        "\n",
        "The policy is updated by $$\\phi \\leftarrow \\phi - \\alpha  \\nabla_\\phi L \\approx \\phi + \\alpha \\nabla_\\phi \\rho(\\pi_\\phi)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz2mpWRWH3re"
      },
      "outputs": [],
      "source": [
        "def discounted_rewards(r, gamma, device, normalize=True):\n",
        "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    discounted_r = np.zeros_like(r)\n",
        "    running_sum = 0\n",
        "    for i in reversed(range(0,len(r))):\n",
        "        discounted_r[i] = running_sum * gamma + r[i]\n",
        "        running_sum = discounted_r[i]\n",
        "\n",
        "    if normalize:\n",
        "      discounted_r = (discounted_r - discounted_r.mean()) / discounted_r.std()\n",
        "\n",
        "    return torch.FloatTensor(discounted_r).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNMkA4FBH3rf"
      },
      "source": [
        "### Main implementation : Policy gradient algorithm\n",
        "\n",
        "Combine all the above steps and implement the policy gradient algorithm with value function baseline in the cell below. The use of baseline is optional."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    env.seed(seed)"
      ],
      "metadata": {
        "id": "MsVjNaQYdLI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm_notebook"
      ],
      "metadata": {
        "id": "v-mxr8_3reIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(policy, state, device):\n",
        "    probs = policy.compute_prob(state)\n",
        "    dist = torch.distributions.Categorical(probs)\n",
        "    a = dist.sample()\n",
        "\n",
        "    #return action\n",
        "    return a.item(), dist.log_prob(a)"
      ],
      "metadata": {
        "id": "20qdxsjdrkTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziiIAobdH3rf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "ed9866cf-f548-4199-9cab-616fdad3f228"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231201_004204-c2q0wntb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/amanichopra/rl-cartpole/runs/c2q0wntb' target=\"_blank\">tough-paper-83</a></strong> to <a href='https://wandb.ai/amanichopra/rl-cartpole' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/amanichopra/rl-cartpole' target=\"_blank\">https://wandb.ai/amanichopra/rl-cartpole</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/amanichopra/rl-cartpole/runs/c2q0wntb' target=\"_blank\">https://wandb.ai/amanichopra/rl-cartpole/runs/c2q0wntb</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#%%wandb\n",
        "#remove the above line if you do not want to see wandb plots in your notebook. You can always see them on the wandb website.\n",
        "\n",
        "#You can change the code in this cell anyway you want\n",
        "#However, just make sure per epsiode reward during the run of this algorithm is being recorded in list rrecord\n",
        "#and logged on wandb as in the last few lines.\n",
        "\n",
        "# parameter initializations (you can change any of these)\n",
        "alpha = 1e-2  # learning rate for PG\n",
        "beta = 1e-2  # learning rate for baseline\n",
        "iterations = 1000  # total num of iterations\n",
        "max_episode_len = 10000\n",
        "envname = \"CartPole-v0\"  # environment name\n",
        "gamma = .99  # discount\n",
        "opt = 'adam'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "normalize_discounted_rewards = True\n",
        "seed = 0\n",
        "\n",
        "# initialize environment\n",
        "env = gym.make(envname)\n",
        "obssize = env.observation_space.low.size\n",
        "actsize = env.action_space.n\n",
        "seed_everything(seed)\n",
        "\n",
        "# initialize networks\n",
        "seed_everything(seed)\n",
        "actor = Policy(obssize, actsize, alpha, device)  # policy initialization: IMPORTANT: this is the policy you will be scored on\n",
        "baseline = ValueFunction(obssize, beta, device)  # baseline initialization\n",
        "\n",
        "run = wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"rl-cartpole\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"method\": \"a2c\",\n",
        "    \"alpha\": alpha,\n",
        "    \"beta\": beta,\n",
        "    \"iterations\": iterations,\n",
        "    \"gamma\": gamma,\n",
        "    \"device\": device,\n",
        "    \"max_episode_len\": max_episode_len,\n",
        "    \"seed\": seed\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To record training reward for logging and plotting purposes\n",
        "rrecord = []\n",
        "for episode in tqdm_notebook(range(iterations)):\n",
        "    state = env.reset()\n",
        "    env.seed(0)\n",
        "\n",
        "    env.action_space.seed(0)\n",
        "    traj = []\n",
        "    score = 0\n",
        "    for step in range(max_episode_len):\n",
        "        action, lp = get_action(actor, state, device)\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        score += reward\n",
        "\n",
        "        traj.append([state, action, reward, lp])\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        state = new_state\n",
        "\n",
        "    rrecord.append(score)\n",
        "\n",
        "    # check if we are done training\n",
        "    #printing moving averages for smoothed visualization.\n",
        "    #Do not change below: this assume you recorded the sum of rewards in each episide in the list rrecord\n",
        "    fixedWindow=100\n",
        "    movingAverage=0\n",
        "    if len(rrecord) >= fixedWindow:\n",
        "        movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
        "        if movingAverage >= 195: # env is solved\n",
        "          break\n",
        "    #wandb logging\n",
        "    wandb.log({ \"training reward\" : rrecord[-1], \"training reward moving average\" : movingAverage})\n",
        "\n",
        "    states = [step[0] for step in traj]\n",
        "    actions = [step[1] for step in traj]\n",
        "    rewards = [step[2] for step in traj]\n",
        "    lps = [step[3] for step in traj]\n",
        "\n",
        "\n",
        "    G = discounted_rewards(rewards, gamma, device)\n",
        "\n",
        "    state_vals = []\n",
        "    for state in states:\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        state_vals.append(baseline.model(state))\n",
        "\n",
        "    state_vals = torch.stack(state_vals).squeeze()\n",
        "\n",
        "    baseline.train(G, state_vals)\n",
        "    ads = [g - v for g, v in zip(G, state_vals)]\n",
        "    ads = torch.tensor(ads).to(device)\n",
        "    actor.train(ads, lps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "386cfd50be28427cbadd119dc07f8868",
            "96c4b8ae433446d69ffa6c1e8ec57838",
            "9cd9ec3d09a04f5093a4b647a0da5aa1",
            "63f288077be24ce494af6dbef1d80e67",
            "50d6383ecd5f46b6a483e51e514364e9",
            "0a8f530ae09348b9bd156c0b4f68063b",
            "de917898a7514faf9a582648d69ad561",
            "2b66f43b3d2f4267bebc9a8a5f2a8d25",
            "f1e5f5c1aa674d5eb19ce03f22c28ee8",
            "7e07057093d7462ba6a0491357f439ee",
            "8ca18784b9d44804928ce13cdb90e94a"
          ]
        },
        "id": "spdqDgsMsWhi",
        "outputId": "dc06d4a0-7280-435b-ef84-8d5d75f485ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-19e610cd24ff>:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for episode in tqdm_notebook(range(iterations)):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "386cfd50be28427cbadd119dc07f8868"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSIhs0tpH3rg"
      },
      "source": [
        "Finally, we evaluate the performance of the trained agent. We will evaluate the performance of the trained policy. The evaluation will be run for 100 epsiodes and print out the average performance across these episodes. Please **do not** change the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir4t6Sw7H3rg"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE CODE HERE\n",
        "\n",
        "### DO NOT CHANGE\n",
        "def evaluate(policy, env, episodes):\n",
        "    # main iteration\n",
        "    score = 0\n",
        "    for episode in range(episodes):\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        rsum = 0\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            p = policy.compute_prob(np.expand_dims(obs,0)).cpu().data.numpy().ravel()\n",
        "            p /= np.sum(p)\n",
        "            action = np.random.choice(np.arange(2), size=1, p=p)[0]\n",
        "\n",
        "            # env stepping forward\n",
        "            newobs, r, done, _ = env.step(action)\n",
        "\n",
        "            # update data\n",
        "            rsum += r\n",
        "            obs = newobs\n",
        "\n",
        "\n",
        "        wandb.log({\"eval reward\" : rsum})\n",
        "        score +=rsum\n",
        "    score = score/episodes\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFP4fexbH3rh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443d1c17-000e-47eb-d311-ffcf15f3d955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval performance of the learned policy: 22.735\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE CODE HERE\n",
        "# after training, we will evaluate the performance of the learned policy \"actor\"\n",
        "# on a target environment\n",
        "env_test = gym.make(envname)\n",
        "eval_episodes = 1000\n",
        "score = evaluate(actor, env_test, eval_episodes)\n",
        "wandb.run.summary[\"score\"]=score\n",
        "print(\"eval performance of the learned policy: {}\".format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZH2a6vdH3rh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387,
          "referenced_widgets": [
            "5335c2d764fc4a3e8a54f9e92b5cb2a9",
            "8f3023ac1be345a28844a5f122fe2874",
            "05fd83821bb5495182b8095e98517d14",
            "b4bf67d387d34f36be30e820c6ea0efc",
            "a167e83b790744d7b56349abc114e4cc",
            "4b82d99b24b94ff2971120652f89a7a8",
            "fb7adefb966745a68cecb81932808f6f",
            "c797bb9cf61643fbb8ead0f5558f55b9"
          ]
        },
        "outputId": "a221d9eb-a027-4e2c-8e43-76fe55eb0518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5335c2d764fc4a3e8a54f9e92b5cb2a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval reward</td><td>▁█▃▁▃█▁▂▂▁▁▆███▁█▃███▅▁▁▁█▁▁▁▆▆▁▂█▁██▁▅▁</td></tr><tr><td>training reward</td><td>▁▁▂▁▁▁▂▃▁▆▄▆▄▆████▅▅▄▄▅▆▆▆▆▇███▂▂███████</td></tr><tr><td>training reward moving average</td><td>▁▁▁▁▁▁▁▁▂▂▃▃▄▅▅▆▆▇▇▇▇▇▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval reward</td><td>200</td></tr><tr><td>score</td><td>198</td></tr><tr><td>training reward</td><td>200.0</td></tr><tr><td>training reward moving average</td><td>194.52525</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">tough-paper-83</strong> at: <a href='https://wandb.ai/amanichopra/rl-cartpole/runs/c2q0wntb' target=\"_blank\">https://wandb.ai/amanichopra/rl-cartpole/runs/c2q0wntb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20231201_004204-c2q0wntb/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "386cfd50be28427cbadd119dc07f8868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96c4b8ae433446d69ffa6c1e8ec57838",
              "IPY_MODEL_9cd9ec3d09a04f5093a4b647a0da5aa1",
              "IPY_MODEL_63f288077be24ce494af6dbef1d80e67"
            ],
            "layout": "IPY_MODEL_50d6383ecd5f46b6a483e51e514364e9"
          }
        },
        "96c4b8ae433446d69ffa6c1e8ec57838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a8f530ae09348b9bd156c0b4f68063b",
            "placeholder": "​",
            "style": "IPY_MODEL_de917898a7514faf9a582648d69ad561",
            "value": " 51%"
          }
        },
        "9cd9ec3d09a04f5093a4b647a0da5aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b66f43b3d2f4267bebc9a8a5f2a8d25",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1e5f5c1aa674d5eb19ce03f22c28ee8",
            "value": 513
          }
        },
        "63f288077be24ce494af6dbef1d80e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e07057093d7462ba6a0491357f439ee",
            "placeholder": "​",
            "style": "IPY_MODEL_8ca18784b9d44804928ce13cdb90e94a",
            "value": " 513/1000 [02:40&lt;04:08,  1.96it/s]"
          }
        },
        "50d6383ecd5f46b6a483e51e514364e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a8f530ae09348b9bd156c0b4f68063b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de917898a7514faf9a582648d69ad561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b66f43b3d2f4267bebc9a8a5f2a8d25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e5f5c1aa674d5eb19ce03f22c28ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e07057093d7462ba6a0491357f439ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ca18784b9d44804928ce13cdb90e94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5335c2d764fc4a3e8a54f9e92b5cb2a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f3023ac1be345a28844a5f122fe2874",
              "IPY_MODEL_05fd83821bb5495182b8095e98517d14"
            ],
            "layout": "IPY_MODEL_b4bf67d387d34f36be30e820c6ea0efc"
          }
        },
        "8f3023ac1be345a28844a5f122fe2874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a167e83b790744d7b56349abc114e4cc",
            "placeholder": "​",
            "style": "IPY_MODEL_4b82d99b24b94ff2971120652f89a7a8",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "05fd83821bb5495182b8095e98517d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb7adefb966745a68cecb81932808f6f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c797bb9cf61643fbb8ead0f5558f55b9",
            "value": 1
          }
        },
        "b4bf67d387d34f36be30e820c6ea0efc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a167e83b790744d7b56349abc114e4cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b82d99b24b94ff2971120652f89a7a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb7adefb966745a68cecb81932808f6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c797bb9cf61643fbb8ead0f5558f55b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}